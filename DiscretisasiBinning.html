
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Binning(Discretisasasi) Menggunakan K-Mean kLastering &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'DiscretisasiBinning';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Klasifikasi Resiko Kesehatan Ibu" href="klasifikasiResikoKematianIbu.html" />
    <link rel="prev" title="Decision Tree" href="DecisionTree.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Selamat datang di notebook penambangan data
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="pemdatD_DataUnderstanding_23-175.html">Understanding Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="DeteksiOutlier.html">Outlier Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="LOF.html">Local Outlier Factor</a></li>
<li class="toctree-l1"><a class="reference internal" href="Naive_Bayes.html">Naive Bayes</a></li>
<li class="toctree-l1"><a class="reference internal" href="23-175_Ahmad%20Dhiyauddin_UTSPendat.html">UTS pendat</a></li>
<li class="toctree-l1"><a class="reference internal" href="K-Mean.html">Klastering</a></li>
<li class="toctree-l1"><a class="reference internal" href="DecisionTree.html">Decision Tree</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Binning(Discretisasasi) Menggunakan K-Mean kLastering</a></li>
<li class="toctree-l1"><a class="reference internal" href="klasifikasiResikoKematianIbu.html">Klasifikasi Resiko Kesehatan Ibu</a></li>





<li class="toctree-l1"><a class="reference internal" href="UAS_Ahmad%20Dhiyauddin_23-175.html">Klasifikasi Resiko Kesehatan Ibu</a></li>





</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FDiscretisasiBinning.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/DiscretisasiBinning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Binning(Discretisasasi) Menggunakan K-Mean kLastering</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#instalasi-library-yang-dibutuhkan">Instalasi library yang dibutuhkan</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualisasi-data-iris-dengan-menyisahkan-kolom-sepal-length">Visualisasi data Iris dengan menyisahkan kolom sepal Length</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#melakukan-klastering-dengan-menggunakan-k-means-klastering">Melakukan Klastering dengan menggunakan K-Means klastering</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tujuan-dan-fungsi-klustering">tujuan dan fungsi klustering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tujuan-dan-fungsi-klaster-clustering">Tujuan dan Fungsi Klaster (Clustering)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tujuan-klastering">🎯 <strong>Tujuan Klastering</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fungsi-klastering-dalam-analisis-data">🛠️ <strong>Fungsi Klastering dalam Analisis Data</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langkah-langkah-algoritma-k-means">Langkah Langkah Algoritma K-Means</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#penjelasan">Penjelasan:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">✅ Penjelasan:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#menggabungkan-data-yang-sudah-di-klastering-menggunakan-k-means-pada-kolom-sepal-length-dengan-kolom-yang-di-drop-tadi-mengembalikan-nilai-sepal-length-berupa-kategorical">Menggabungkan data yang sudah di klastering menggunakan K-Means pada kolom Sepal Length dengan kolom yang di drop tadi mengembalikan nilai Sepal Length berupa kategorical</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#melakukan-klastering-pada-semua-fitur-menggunakan-k-means-dengan-4-klaster-untuk-fitur-sepal-length-dan-petal-length-dan-3-klaster-untuk-sepal-width-dan-petal-width">Melakukan klastering pada semua fitur menggunakan K-Means dengan 4 Klaster untuk fitur Sepal Length dan Petal Length dan 3 klaster untuk Sepal Width dan Petal Width</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#klasifikasi-naive-bayes-untuk-data-kategorikal">🌼 Klasifikasi Naive Bayes untuk Data Kategorikal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#teorema-bayes">📘 Teorema Bayes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inisialisa-library">Inisialisa Library</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inisialisasi-data-fitur-kategorical">Inisialisasi data fitur kategorical</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ubah-data-huruf-menjadi-angka-0-3-untuk-4-klaster-dan-0-2-untuk-3-klaster">ubah data Huruf menjadi angka 0-3 untuk 4 klaster dan 0-2 untuk 3 klaster</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latih-data-dan-prediksi-menggunakan-naive-bayes">Latih data dan Prediksi menggunakan Naive Bayes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hitung-akurasinya">Hitung Akurasinya</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#menggunakan-fitur-numerik">menggunakan fitur numerik</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kesimpulan-perbandingan-akurasi-dari-klasifikasi-menggunakan-naive-bayes-dengan-fitur-kategorical-dan-numerik">Kesimpulan perbandingan akurasi dari klasifikasi menggunakan naive bayes dengan fitur kategorical dan numerik</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kesimpulan">✅ Kesimpulan:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-klasifikasi-dengan-decision-tree-categorical">STEP-BY-STEP KLASIFIKASI DENGAN DECISION TREE (CATEGORICAL)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tampilkan-data-keseluruhan-yang-sudah-menjadi-kategorical">Tampilkan data keseluruhan yang sudah menjadi Kategorical</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mengganti-kolom-huruf-menjadi-angka">Mengganti kolom Huruf menjadi Angka</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#membagi-data-train-dan-data-test-kemudian-melatih-model-tersebut">Membagi data train dan data test kemudian melatih model tersebut</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#langkah-langkah-klasifikasi-decision-tree-fitur-kategorikal-03">🌳 <strong>Langkah-Langkah Klasifikasi Decision Tree (Fitur Kategorikal 0–3)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#menentukan-tujuan">✅ <strong>1. Menentukan Tujuan</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#hitung-entropy-dataset-awal-root-node">🔢 <strong>2. Hitung Entropy Dataset Awal (Root Node)</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#lakukan-split-pada-tiap-fitur">✂️ <strong>3. Lakukan Split pada Tiap Fitur</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#hitung-split-entropy-entropy-setelah-split">📘 <strong>4. Hitung Split Entropy (Entropy Setelah Split)</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#hitung-information-gain">🔍 <strong>5. Hitung Information Gain</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#pilih-split-terbaik-threshold">🔪 <strong>6. Pilih Split Terbaik (Threshold)</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#buat-node-baru-dan-ulangi-rekursif">🌿 <strong>7. Buat Node Baru dan Ulangi Rekursif</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#simbol-dan-penjelasan">📌 Simbol dan Penjelasan</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ilustrasi-decision-tree-dengan-fitur-kategorikal-03">✅ Ilustrasi: Decision Tree dengan Fitur Kategorikal 0–3</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualisasi-decision-tree">Visualisasi Decision Tree</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Hitung Akurasinya</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kesimpulan-perbandingan-akurasi-pada-klasifikasi-data-iris-dengan-menggunakan-decision-tree-antara-data-numerik-dan-kategorical">Kesimpulan Perbandingan Akurasi pada klasifikasi data Iris dengan menggunakan Decision Tree antara data numerik dan kategorical</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kesimpulan-perbandingan-akurasi-decision-tree-pada-data-iris">📊 Kesimpulan Perbandingan Akurasi Decision Tree pada Data Iris</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">✅ Kesimpulan:</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="binning-discretisasasi-menggunakan-k-mean-klastering">
<h1>Binning(Discretisasasi) Menggunakan K-Mean kLastering<a class="headerlink" href="#binning-discretisasasi-menggunakan-k-mean-klastering" title="Link to this heading">#</a></h1>
<p>Binning (Discretization) adalah proses mengubah fitur numerik kontinu menjadi fitur kategorikal atau ordinal. Misalnya, nilai umur bisa dikelompokkan menjadi “Muda”, “Dewasa”, dan “Lansia”. Tujuannya adalah untuk:</p>
<ul class="simple">
<li><p>Mengurangi kompleksitas data.</p></li>
<li><p>Menangani outlier.</p></li>
<li><p>Memenuhi prasyarat model tertentu (misalnya Naive Bayes).</p></li>
<li><p>Memberikan interpretasi yang lebih mudah.</p></li>
</ul>
<p>Mengapa Gunakan K-Means?</p>
<p>Metode klasik seperti equal-width atau equal-frequency tidak memperhitungkan sebaran data. K-Means Clustering lebih adaptif karena membentuk cluster berdasarkan pola alami distribusi data.</p>
<section id="instalasi-library-yang-dibutuhkan">
<h2>Instalasi library yang dibutuhkan<a class="headerlink" href="#instalasi-library-yang-dibutuhkan" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>pymysql
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>psycopg2-binary
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>pandas<span class="w"> </span>tabulate
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>scikit-fuzzy
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: pymysql in /usr/local/lib/python3.11/dist-packages (1.1.1)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Collecting psycopg2-binary
  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)
Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)
?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">0.0/3.0 MB</span> <span class=" -Color -Color-Red">?</span> eta <span class=" -Color -Color-Cyan">-:--:--</span>
   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">0.2/3.0 MB</span> <span class=" -Color -Color-Red">4.7 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:01</span>
   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">1.3/3.0 MB</span> <span class=" -Color -Color-Red">18.7 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:01</span>
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">3.0/3.0 MB</span> <span class=" -Color -Color-Red">29.9 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
?25h
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Installing collected packages: psycopg2-binary
Successfully installed psycopg2-binary-2.9.10
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)
Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)
Requirement already satisfied: numpy&gt;=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)
Requirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)
Requirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.17.0)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Collecting scikit-fuzzy
  Downloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)
Downloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl (920 kB)
?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">0.0/920.8 kB</span> <span class=" -Color -Color-Red">?</span> eta <span class=" -Color -Color-Cyan">-:--:--</span>
   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">153.6/920.8 kB</span> <span class=" -Color -Color-Red">4.6 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:01</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ <span class=" -Color -Color-Green">911.4/920.8 kB</span> <span class=" -Color -Color-Red">14.7 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:01</span>
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">920.8/920.8 kB</span> <span class=" -Color -Color-Red">12.3 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
?25h
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Installing collected packages: scikit-fuzzy
Successfully installed scikit-fuzzy-0.5.0
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualisasi-data-iris-dengan-menyisahkan-kolom-sepal-length">
<h2>Visualisasi data Iris dengan menyisahkan kolom sepal Length<a class="headerlink" href="#visualisasi-data-iris-dengan-menyisahkan-kolom-sepal-length" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pymysql</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tabulate</span><span class="w"> </span><span class="kn">import</span> <span class="n">tabulate</span>

<span class="c1"># Konfigurasi koneksi ke database MySQL</span>
<span class="n">DB_HOST</span> <span class="o">=</span> <span class="s2">&quot;mysql-pendata23-175-mysqlpendata23-175.g.aivencloud.com&quot;</span>
<span class="n">DB_PORT</span> <span class="o">=</span> <span class="mi">17974</span>
<span class="n">DB_NAME</span> <span class="o">=</span> <span class="s2">&quot;defaultdb&quot;</span>
<span class="n">DB_USER</span> <span class="o">=</span> <span class="s2">&quot;avnadmin&quot;</span>
<span class="n">DB_PASS</span> <span class="o">=</span> <span class="s2">&quot;AVNS_756YWhIqpe1WzSKsGNx&quot;</span>

<span class="k">try</span><span class="p">:</span>
    <span class="c1"># Membuat koneksi ke MySQL</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">pymysql</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span>
        <span class="n">host</span><span class="o">=</span><span class="n">DB_HOST</span><span class="p">,</span>
        <span class="n">port</span><span class="o">=</span><span class="n">DB_PORT</span><span class="p">,</span>
        <span class="n">user</span><span class="o">=</span><span class="n">DB_USER</span><span class="p">,</span>
        <span class="n">password</span><span class="o">=</span><span class="n">DB_PASS</span><span class="p">,</span>
        <span class="n">database</span><span class="o">=</span><span class="n">DB_NAME</span><span class="p">,</span>
        <span class="n">cursorclass</span><span class="o">=</span><span class="n">pymysql</span><span class="o">.</span><span class="n">cursors</span><span class="o">.</span><span class="n">DictCursor</span>  <span class="c1"># Mengembalikan hasil sebagai dictionary</span>
    <span class="p">)</span>
    <span class="n">cur</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span>

    <span class="c1"># Menjalankan query SQL</span>
    <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;SELECT * FROM irisno.Iris ORDER BY id ASC;&quot;</span>  <span class="c1"># Sesuaikan dengan nama tabel</span>
    <span class="n">cur</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="n">cur</span><span class="o">.</span><span class="n">fetchall</span><span class="p">()</span>

    <span class="c1"># Jika tabel kosong</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">rows</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tabel kosong, tidak ada data untuk ditampilkan.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Mengubah hasil query menjadi DataFrame pandas</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>

        <span class="c1"># Memastikan kolom &#39;id&#39; dan &#39;sepal_length&#39; ada sebelum memproses</span>
        <span class="n">required_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Id&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal length&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">required_columns</span><span class="p">):</span>
            <span class="c1"># Menyimpan hanya kolom &#39;id&#39; dan &#39;sepal_length&#39;</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">required_columns</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Salah satu atau lebih kolom yang diperlukan (</span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">required_columns</span><span class="p">)</span><span class="si">}</span><span class="s2">) tidak ditemukan dalam tabel.&quot;</span><span class="p">)</span>
            <span class="c1"># Buat DataFrame kosong dengan kolom yang dibutuhkan jika tidak ditemukan</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">required_columns</span><span class="p">)</span>

        <span class="c1"># Menampilkan 3 data awal, titik-titik di tengah, dan 3 data akhir</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">df</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
            <span class="n">df_selected</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="s2">&quot;...&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)],</span> <span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">df</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">3</span><span class="p">)])</span>
            <span class="c1"># Menampilkan tabel dengan format psql</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span><span class="n">df_selected</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s2">&quot;keys&quot;</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s2">&quot;psql&quot;</span><span class="p">,</span> <span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tidak ada data untuk ditampilkan setelah filter kolom.&quot;</span><span class="p">)</span>

<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

<span class="k">finally</span><span class="p">:</span>
    <span class="c1"># Menutup koneksi ke database</span>
    <span class="k">if</span> <span class="n">cur</span><span class="p">:</span>
        <span class="n">cur</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">conn</span><span class="p">:</span>
        <span class="n">conn</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+------+----------------+
| Id   | sepal length   |
|------+----------------|
| 1    | 5.1            |
| 2    | 4.9            |
| 3    | 4.7            |
| ...  | ...            |
| 148  | 6.5            |
| 149  | 6.2            |
| 150  | 5.9            |
+------+----------------+
</pre></div>
</div>
</div>
</div>
</section>
<section id="melakukan-klastering-dengan-menggunakan-k-means-klastering">
<h2>Melakukan Klastering dengan menggunakan K-Means klastering<a class="headerlink" href="#melakukan-klastering-dengan-menggunakan-k-means-klastering" title="Link to this heading">#</a></h2>
<p>K-Means adalah Teknik pengelompokan (clustering) berbasis partisi membagi data menjadi K kelompok berdasarkan jarak setiap kelompok direpresentasikan oleh centroid(rata-rata jarak dari titik dalam kluster) atau biasa disebut titik pusat suatu klaster.</p>
<p>Data yang sering kali tidak memiliki label–&gt;Unsupervised learning clustering membantu mengelompokkan data berdasarkan kesamaan karakteristik</p>
<section id="tujuan-dan-fungsi-klustering">
<h3>tujuan dan fungsi klustering<a class="headerlink" href="#tujuan-dan-fungsi-klustering" title="Link to this heading">#</a></h3>
</section>
<section id="tujuan-dan-fungsi-klaster-clustering">
<h3>Tujuan dan Fungsi Klaster (Clustering)<a class="headerlink" href="#tujuan-dan-fungsi-klaster-clustering" title="Link to this heading">#</a></h3>
<p><strong>Clustering</strong> adalah salah satu teknik utama dalam <strong>unsupervised learning</strong> yang bertujuan untuk mengelompokkan data berdasarkan kemiripan atau kedekatan karakteristik tanpa menggunakan label atau target output. Klastering sering digunakan dalam data mining, analisis data eksploratif, segmentasi pasar, dan banyak bidang lain.</p>
<hr class="docutils" />
<section id="tujuan-klastering">
<h4>🎯 <strong>Tujuan Klastering</strong><a class="headerlink" href="#tujuan-klastering" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Mengelompokkan Data Serupa</strong>
Tujuan utama klastering adalah mengelompokkan data ke dalam beberapa <strong>kelompok (klaster)</strong> sedemikian rupa sehingga:</p>
<ul class="simple">
<li><p>Data dalam satu klaster <strong>mirip</strong> satu sama lain (intra-cluster similarity tinggi).</p></li>
<li><p>Data antar klaster <strong>berbeda jauh</strong> (inter-cluster dissimilarity tinggi).</p></li>
</ul>
</li>
<li><p><strong>Reduksi Kompleksitas Data</strong>
Dengan membagi data ke dalam klaster, kita bisa menyederhanakan analisis data, terutama saat bekerja dengan data besar dan kompleks.</p></li>
<li><p><strong>Mengidentifikasi Pola atau Struktur Tersembunyi</strong>
Klastering dapat membantu mengungkap pola tersembunyi dalam data yang mungkin tidak tampak secara langsung.</p></li>
<li><p><strong>Segmentasi</strong>
Dalam dunia bisnis dan pemasaran, klastering digunakan untuk membagi pelanggan ke dalam segmen berdasarkan perilaku atau atribut tertentu (misalnya: segmentasi pelanggan, segmentasi wilayah, dll).</p></li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="fungsi-klastering-dalam-analisis-data">
<h3>🛠️ <strong>Fungsi Klastering dalam Analisis Data</strong><a class="headerlink" href="#fungsi-klastering-dalam-analisis-data" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Fungsi</p></th>
<th class="head"><p>Penjelasan</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Eksplorasi Data</strong></p></td>
<td><p>Klaster membantu memahami struktur data, mendeteksi outlier, dan mengidentifikasi grup penting.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Pra-pemrosesan</strong></p></td>
<td><p>Dapat digunakan sebelum algoritma supervised learning untuk menyaring atau mengelompokkan data.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Rekomendasi Sistem</strong></p></td>
<td><p>Sistem rekomendasi sering menggunakan klastering untuk menemukan grup pengguna dengan minat serupa.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Pengelompokan Dokumen / Teks</strong></p></td>
<td><p>Untuk mengelompokkan artikel, berita, atau dokumen berdasarkan topik utama.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Deteksi Anomali</strong></p></td>
<td><p>Data yang tidak masuk ke klaster manapun bisa dianggap sebagai anomali atau outlier.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Biologi / Genetika</strong></p></td>
<td><p>Untuk mengelompokkan gen, ekspresi gen, atau jenis sel berdasarkan karakteristik biologis.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="langkah-langkah-algoritma-k-means">
<h3>Langkah Langkah Algoritma K-Means<a class="headerlink" href="#langkah-langkah-algoritma-k-means" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Tentukan jumlah cluster
Disini saya akan mencoba dengan data yang ditampilkan membaginya dengan 2,3,4 klaster</p></li>
<li><p>Pilih K centroid awal secara acak setiap klaster memiliki 1 centroid</p></li>
<li><p>Hitung jarak setiap data ke tiap centroid
gunakan rumus Encludian Distance
$<span class="math notranslate nohighlight">\(
d(\mathbf{x}, \mathbf{c}) = \sqrt{ \sum_{i=1}^{n} (x_i - c_i)^2 }
\)</span>$</p></li>
</ol>
</section>
<section id="penjelasan">
<h3>Penjelasan:<a class="headerlink" href="#penjelasan" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, x_2, \ldots, x_n)\)</span>: vektor data.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{c} = (c_1, c_2, \ldots, c_n)\)</span>: vektor centroid.</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span>: jumlah dimensi atau fitur.</p></li>
<li><p><span class="math notranslate nohighlight">\(d(\mathbf{x}, \mathbf{c})\)</span>: jarak Euclidean antara data dan centroid.</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p>Kelompokan data dengan centroid terdekat</p></li>
<li><p>Hitung ulang centroid dari rata rata anggota kluster
$<span class="math notranslate nohighlight">\(
\mu_k = \frac{1}{N_k} \sum_{x_i \in C_k} x_i
\)</span>$</p></li>
</ol>
</section>
<section id="id1">
<h3>✅ Penjelasan:<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>$\mu_k$</strong>
Menyatakan <strong>centroid (titik pusat)</strong> dari klaster ke-<strong>$k$</strong>.</p></li>
<li><p><strong>$N_k$</strong>
Jumlah <strong>data (anggota)</strong> yang terdapat dalam klaster ke-<strong>$k$</strong>.</p></li>
<li><p><strong>$x_i \in C_k$</strong>
Menyatakan bahwa <strong>$x_i$</strong> adalah titik data yang termasuk ke dalam klaster <strong>$C_k$</strong>.</p></li>
<li><p><strong>$\sum_{x_i \in C_k} x_i$</strong>
Menyatakan jumlah (penjumlahan vektor) dari seluruh titik data yang berada di dalam klaster <strong>$C_k$</strong>.</p></li>
<li><p><strong>$\frac{1}{N_k}$</strong>
Digunakan untuk menghitung rata-rata dari semua anggota dalam klaster tersebut.</p></li>
</ul>
<ol class="arabic simple" start="6">
<li><p>Ulangi langkah 3-5 hingga konvergen(tidak ada perubahan signifikan)</p></li>
</ol>
<p>Pada Kolom sepal Length ini dibagi kedalam 4 klaster yaitu A,B,C,D</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pymysql</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tabulate</span><span class="w"> </span><span class="kn">import</span> <span class="n">tabulate</span> <span class="c1"># Import tabulate untuk menampilkan tabel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span> <span class="c1"># Diperlukan untuk plt.show(), plt.scatter, dll.</span>

<span class="c1"># --- Konfigurasi Koneksi ke Database MySQL ---</span>
<span class="n">DB_HOST</span> <span class="o">=</span> <span class="s2">&quot;mysql-pendata23-175-mysqlpendata23-175.g.aivencloud.com&quot;</span>
<span class="n">DB_PORT</span> <span class="o">=</span> <span class="mi">17974</span>
<span class="n">DB_NAME</span> <span class="o">=</span> <span class="s2">&quot;defaultdb&quot;</span>
<span class="n">DB_USER</span> <span class="o">=</span> <span class="s2">&quot;avnadmin&quot;</span>
<span class="n">DB_PASS</span> <span class="o">=</span> <span class="s2">&quot;AVNS_756YWhIqpe1WzSKsGNx&quot;</span>

<span class="k">try</span><span class="p">:</span>
    <span class="c1"># Membuat koneksi ke MySQL</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">pymysql</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span>
        <span class="n">host</span><span class="o">=</span><span class="n">DB_HOST</span><span class="p">,</span>
        <span class="n">port</span><span class="o">=</span><span class="n">DB_PORT</span><span class="p">,</span>
        <span class="n">user</span><span class="o">=</span><span class="n">DB_USER</span><span class="p">,</span>
        <span class="n">password</span><span class="o">=</span><span class="n">DB_PASS</span><span class="p">,</span>
        <span class="n">database</span><span class="o">=</span><span class="n">DB_NAME</span><span class="p">,</span>
        <span class="n">cursorclass</span><span class="o">=</span><span class="n">pymysql</span><span class="o">.</span><span class="n">cursors</span><span class="o">.</span><span class="n">DictCursor</span> <span class="c1"># Mengembalikan hasil sebagai dictionary</span>
    <span class="p">)</span>
    <span class="n">cur</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span>

    <span class="c1"># Menjalankan query SQL untuk mengambil semua kolom (`SELECT *`)</span>
    <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;SELECT * FROM irisno.Iris ORDER BY id ASC;&quot;</span>
    <span class="n">cur</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="n">cur</span><span class="o">.</span><span class="n">fetchall</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">rows</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tabel kosong, tidak ada data untuk diproses.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Mengubah hasil query menjadi DataFrame pandas</span>
        <span class="c1"># df_raw_from_db akan mempertahankan nama kolom asli dari database (misal: &#39;sepal length&#39;)</span>
        <span class="n">df_raw_from_db</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>

        <span class="c1"># Buat salinan DataFrame untuk proses klastering dan pengubahan nama kolom agar lebih mudah diakses di Python</span>
        <span class="n">df_for_clustering</span> <span class="o">=</span> <span class="n">df_raw_from_db</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="c1"># Mengubah &#39;sepal length&#39; menjadi &#39;sepal_length&#39; di df_for_clustering untuk kemudahan akses Python</span>
        <span class="k">if</span> <span class="s1">&#39;sepal length&#39;</span> <span class="ow">in</span> <span class="n">df_for_clustering</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="n">df_for_clustering</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sepal length&#39;</span><span class="p">:</span> <span class="s1">&#39;sepal_length&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Ambil fitur numerik untuk klastering: hanya &#39;sepal_length&#39; dari df_for_clustering</span>
        <span class="n">val_X</span> <span class="o">=</span> <span class="n">df_for_clustering</span><span class="p">[[</span><span class="s1">&#39;sepal_length&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>

        <span class="c1"># --- KMeans Model untuk 4 Klaster ---</span>
        <span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># Menentukan jumlah klaster</span>
        <span class="n">k_means_model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span>
            <span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">,</span>
            <span class="n">init</span><span class="o">=</span><span class="s1">&#39;k-means++&#39;</span><span class="p">,</span> <span class="c1"># Metode inisialisasi centroid</span>
            <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>        <span class="c1"># Jumlah kali algoritma K-Means dijalankan</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span>
            <span class="n">tol</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>  <span class="c1"># Untuk hasil yang reproducible</span>
            <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;lloyd&#39;</span> <span class="c1"># Algoritma K-Means yang digunakan</span>
        <span class="p">)</span>

        <span class="c1"># Latih model K-Means</span>
        <span class="n">k_means_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">val_X</span><span class="p">)</span>

        <span class="c1"># Ambil label klaster untuk setiap data point</span>
        <span class="n">cluster_labels_numeric</span> <span class="o">=</span> <span class="n">k_means_model</span><span class="o">.</span><span class="n">labels_</span> <span class="c1"># Simpan label numerik asli</span>
        <span class="c1"># Ambil koordinat centroid dari setiap klaster</span>
        <span class="n">cluster_centroids</span> <span class="o">=</span> <span class="n">k_means_model</span><span class="o">.</span><span class="n">cluster_centers_</span>

        <span class="c1"># --- Konversi Label Klaster Numerik ke Huruf (A, B, C, D) ---</span>
        <span class="n">label_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="s1">&#39;D&#39;</span><span class="p">}</span>
        <span class="n">cluster_labels_alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">label_mapping</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">cluster_labels_numeric</span><span class="p">])</span>

        <span class="c1"># --- Ganti nama kolom &#39;id&#39; menjadi &#39;Id&#39; di DataFrame asli (df_raw_from_db) ---</span>
        <span class="k">if</span> <span class="s1">&#39;id&#39;</span> <span class="ow">in</span> <span class="n">df_raw_from_db</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="n">df_raw_from_db</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;id&#39;</span><span class="p">:</span> <span class="s1">&#39;Id&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># --- Ganti kolom &#39;sepal length&#39; dengan hasil klaster dan ubah namanya ---</span>
        <span class="k">if</span> <span class="s1">&#39;sepal length&#39;</span> <span class="ow">in</span> <span class="n">df_raw_from_db</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="n">df_raw_from_db</span><span class="p">[</span><span class="s1">&#39;sepal length&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_labels_alpha</span> <span class="c1"># Timpa kolom asli dengan label klaster</span>
            <span class="c1"># Ubah nama kolomnya menjadi &#39;Sepal Length Klaster&#39; untuk kejelasan</span>
            <span class="n">df_raw_from_db</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sepal length&#39;</span><span class="p">:</span> <span class="s1">&#39;Sepal Length&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Data hasil klaster disimpan dalam variabel &#39;df_raw_from_db&#39; dengan label A-D.&quot;</span><span class="p">)</span>

        <span class="c1"># --- Visualisasi Hasil Klaster ---</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
        <span class="n">colors</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">n_clusters</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">):</span>
            <span class="n">y_jitter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">val_X</span><span class="p">[</span><span class="n">cluster_labels_numeric</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">-</span> <span class="mf">0.05</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
                <span class="n">val_X</span><span class="p">[</span><span class="n">cluster_labels_numeric</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>  <span class="c1"># Sumbu X adalah &#39;sepal_length&#39; (dari df_for_clustering)</span>
                <span class="n">y_jitter</span><span class="p">,</span>                              <span class="c1"># Sumbu Y adalah nilai jitter</span>
                <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Cluster </span><span class="si">{</span><span class="n">label_mapping</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span> <span class="c1"># Gunakan label huruf pada legenda</span>
            <span class="p">)</span>

        <span class="c1"># Plot centroid</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
            <span class="n">cluster_centroids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="c1"># Sumbu X adalah posisi centroid</span>
            <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">cluster_centroids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="c1"># Sumbu Y centroid berada di 0 (tanpa jitter)</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
            <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
            <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span>
            <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Centroid&#39;</span>
        <span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;KMeans Clustering (k=</span><span class="si">{</span><span class="n">n_clusters</span><span class="si">}</span><span class="s1">) pada Sepal Length&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal Length&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Jitter (untuk visualisasi sebaran)&#39;</span><span class="p">)</span> <span class="c1"># Label sumbu Y yang menjelaskan jitter</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

        <span class="c1"># --- Tampilkan Nilai Centroid ---</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Nilai Centroid untuk setiap klaster:&quot;</span><span class="p">)</span>
        <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">cluster_centroids</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sorted_indices</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cluster </span><span class="si">{</span><span class="n">label_mapping</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">cluster_centroids</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># --- Tampilkan Id dan Kolom Klaster &#39;Sepal Length Klaster&#39; ---</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Id dan Data Klaster Sepal Length:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span><span class="n">df_raw_from_db</span><span class="p">[[</span><span class="s1">&#39;Id&#39;</span><span class="p">,</span> <span class="s1">&#39;Sepal Length&#39;</span><span class="p">]],</span> <span class="n">headers</span><span class="o">=</span><span class="s1">&#39;keys&#39;</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s1">&#39;psql&#39;</span><span class="p">,</span> <span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Terjadi Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">finally</span><span class="p">:</span>
    <span class="c1"># Menutup koneksi ke database</span>
    <span class="k">if</span> <span class="s1">&#39;cur&#39;</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">()</span> <span class="ow">and</span> <span class="n">cur</span><span class="p">:</span>
        <span class="n">cur</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">if</span> <span class="s1">&#39;conn&#39;</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">()</span> <span class="ow">and</span> <span class="n">conn</span><span class="p">:</span>
        <span class="n">conn</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data hasil klaster disimpan dalam variabel &#39;df_raw_from_db&#39; dengan label A-D.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipython-input-3-1129553682.py:86: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.
  colors = plt.cm.get_cmap(&#39;viridis&#39;, n_clusters)
</pre></div>
</div>
<img alt="_images/ab69159176553abb74beeb08ec1ad5c002fbade3de26c49c60ef3ac090c197d9.png" src="_images/ab69159176553abb74beeb08ec1ad5c002fbade3de26c49c60ef3ac090c197d9.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nilai Centroid untuk setiap klaster:
Cluster A: 4.8957
Cluster B: 5.7347
Cluster C: 6.5256
Cluster D: 7.4750

Id dan Data Klaster Sepal Length:
+------+----------------+
|   Id | Sepal Length   |
|------+----------------|
|    1 | C              |
|    2 | C              |
|    3 | C              |
|    4 | C              |
|    5 | C              |
|    6 | A              |
|    7 | C              |
|    8 | C              |
|    9 | C              |
|   10 | C              |
|   11 | A              |
|   12 | C              |
|   13 | C              |
|   14 | C              |
|   15 | A              |
|   16 | A              |
|   17 | A              |
|   18 | C              |
|   19 | A              |
|   20 | C              |
|   21 | A              |
|   22 | C              |
|   23 | C              |
|   24 | C              |
|   25 | C              |
|   26 | C              |
|   27 | C              |
|   28 | C              |
|   29 | C              |
|   30 | C              |
|   31 | C              |
|   32 | A              |
|   33 | C              |
|   34 | A              |
|   35 | C              |
|   36 | C              |
|   37 | A              |
|   38 | C              |
|   39 | C              |
|   40 | C              |
|   41 | C              |
|   42 | C              |
|   43 | C              |
|   44 | C              |
|   45 | C              |
|   46 | C              |
|   47 | C              |
|   48 | C              |
|   49 | C              |
|   50 | C              |
|   51 | D              |
|   52 | D              |
|   53 | D              |
|   54 | A              |
|   55 | D              |
|   56 | A              |
|   57 | D              |
|   58 | C              |
|   59 | D              |
|   60 | C              |
|   61 | C              |
|   62 | A              |
|   63 | A              |
|   64 | A              |
|   65 | A              |
|   66 | D              |
|   67 | A              |
|   68 | A              |
|   69 | D              |
|   70 | A              |
|   71 | A              |
|   72 | A              |
|   73 | D              |
|   74 | A              |
|   75 | D              |
|   76 | D              |
|   77 | D              |
|   78 | D              |
|   79 | A              |
|   80 | A              |
|   81 | A              |
|   82 | A              |
|   83 | A              |
|   84 | A              |
|   85 | A              |
|   86 | A              |
|   87 | D              |
|   88 | D              |
|   89 | A              |
|   90 | A              |
|   91 | A              |
|   92 | A              |
|   93 | A              |
|   94 | C              |
|   95 | A              |
|   96 | A              |
|   97 | A              |
|   98 | D              |
|   99 | C              |
|  100 | A              |
|  101 | D              |
|  102 | A              |
|  103 | B              |
|  104 | D              |
|  105 | D              |
|  106 | B              |
|  107 | C              |
|  108 | B              |
|  109 | D              |
|  110 | B              |
|  111 | D              |
|  112 | D              |
|  113 | D              |
|  114 | A              |
|  115 | A              |
|  116 | D              |
|  117 | D              |
|  118 | B              |
|  119 | B              |
|  120 | A              |
|  121 | D              |
|  122 | A              |
|  123 | B              |
|  124 | D              |
|  125 | D              |
|  126 | B              |
|  127 | D              |
|  128 | A              |
|  129 | D              |
|  130 | B              |
|  131 | B              |
|  132 | B              |
|  133 | D              |
|  134 | D              |
|  135 | A              |
|  136 | B              |
|  137 | D              |
|  138 | D              |
|  139 | A              |
|  140 | D              |
|  141 | D              |
|  142 | D              |
|  143 | A              |
|  144 | D              |
|  145 | D              |
|  146 | D              |
|  147 | D              |
|  148 | D              |
|  149 | D              |
|  150 | A              |
+------+----------------+
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="menggabungkan-data-yang-sudah-di-klastering-menggunakan-k-means-pada-kolom-sepal-length-dengan-kolom-yang-di-drop-tadi-mengembalikan-nilai-sepal-length-berupa-kategorical">
<h2>Menggabungkan data yang sudah di klastering menggunakan K-Means pada kolom Sepal Length dengan kolom yang di drop tadi mengembalikan nilai Sepal Length berupa kategorical<a class="headerlink" href="#menggabungkan-data-yang-sudah-di-klastering-menggunakan-k-means-pada-kolom-sepal-length-dengan-kolom-yang-di-drop-tadi-mengembalikan-nilai-sepal-length-berupa-kategorical" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pymysql</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tabulate</span><span class="w"> </span><span class="kn">import</span> <span class="n">tabulate</span> <span class="c1"># Import tabulate untuk menampilkan tabel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>

<span class="c1"># --- Konfigurasi Koneksi ke Database MySQL ---</span>
<span class="n">DB_HOST</span> <span class="o">=</span> <span class="s2">&quot;mysql-pendata23-175-mysqlpendata23-175.g.aivencloud.com&quot;</span>
<span class="n">DB_PORT</span> <span class="o">=</span> <span class="mi">17974</span>
<span class="n">DB_NAME</span> <span class="o">=</span> <span class="s2">&quot;defaultdb&quot;</span>
<span class="n">DB_USER</span> <span class="o">=</span> <span class="s2">&quot;avnadmin&quot;</span>
<span class="n">DB_PASS</span> <span class="o">=</span> <span class="s2">&quot;AVNS_756YWhIqpe1WzSKsGNx&quot;</span>

<span class="n">df_original_full</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span> <span class="c1"># Inisialisasi DataFrame kosong</span>

<span class="k">try</span><span class="p">:</span>
    <span class="c1"># Membuat koneksi ke MySQL</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">pymysql</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span>
        <span class="n">host</span><span class="o">=</span><span class="n">DB_HOST</span><span class="p">,</span>
        <span class="n">port</span><span class="o">=</span><span class="n">DB_PORT</span><span class="p">,</span>
        <span class="n">user</span><span class="o">=</span><span class="n">DB_USER</span><span class="p">,</span>
        <span class="n">password</span><span class="o">=</span><span class="n">DB_PASS</span><span class="p">,</span>
        <span class="n">database</span><span class="o">=</span><span class="n">DB_NAME</span><span class="p">,</span>
        <span class="n">cursorclass</span><span class="o">=</span><span class="n">pymysql</span><span class="o">.</span><span class="n">cursors</span><span class="o">.</span><span class="n">DictCursor</span> <span class="c1"># Mengembalikan hasil sebagai dictionary</span>
    <span class="p">)</span>
    <span class="n">cur</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span>

    <span class="c1"># Menjalankan query SQL untuk mengambil semua kolom yang relevan</span>
    <span class="c1"># Pastikan nama kolom sesuai dengan yang ada di database Anda</span>
    <span class="c1"># Jika ada spasi di nama kolom, gunakan backticks (` `)</span>
    <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;SELECT id, `sepal length`, `sepal width`, `petal length`, `petal width` FROM irisno.Iris ORDER BY id ASC;&quot;</span>
    <span class="n">cur</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="n">cur</span><span class="o">.</span><span class="n">fetchall</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">rows</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tabel kosong, tidak ada data untuk diproses.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Mengubah hasil query menjadi DataFrame pandas</span>
        <span class="n">df_original_full</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>

        <span class="c1"># Pastikan nama kolom diubah menjadi format tanpa spasi agar mudah diakses di Python</span>
        <span class="k">if</span> <span class="s1">&#39;sepal length&#39;</span> <span class="ow">in</span> <span class="n">df_original_full</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="n">df_original_full</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sepal length&#39;</span><span class="p">:</span> <span class="s1">&#39;sepal_length&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="s1">&#39;sepal width&#39;</span> <span class="ow">in</span> <span class="n">df_original_full</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="n">df_original_full</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sepal width&#39;</span><span class="p">:</span> <span class="s1">&#39;sepal_width&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="s1">&#39;petal length&#39;</span> <span class="ow">in</span> <span class="n">df_original_full</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="n">df_original_full</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;petal length&#39;</span><span class="p">:</span> <span class="s1">&#39;petal_length&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="s1">&#39;petal width&#39;</span> <span class="ow">in</span> <span class="n">df_original_full</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="n">df_original_full</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;petal width&#39;</span><span class="p">:</span> <span class="s1">&#39;petal_width&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Ambil fitur numerik untuk klastering: hanya &#39;sepal_length&#39;</span>
        <span class="n">val_X</span> <span class="o">=</span> <span class="n">df_original_full</span><span class="p">[[</span><span class="s1">&#39;sepal_length&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>

        <span class="c1"># --- KMeans Model untuk 4 Klaster ---</span>
        <span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">4</span>
        <span class="n">k_means_model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span>
            <span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">,</span>
            <span class="n">init</span><span class="o">=</span><span class="s1">&#39;k-means++&#39;</span><span class="p">,</span>
            <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span>
            <span class="n">tol</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
            <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;lloyd&#39;</span>
        <span class="p">)</span>

        <span class="c1"># Latih model K-Means</span>
        <span class="n">k_means_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">val_X</span><span class="p">)</span>

        <span class="c1"># Ambil label klaster numerik</span>
        <span class="n">cluster_labels_numeric</span> <span class="o">=</span> <span class="n">k_means_model</span><span class="o">.</span><span class="n">labels_</span>

        <span class="c1"># --- Konversi Label Klaster Numerik ke Huruf (A, B, C, D) ---</span>
        <span class="n">label_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="s1">&#39;D&#39;</span><span class="p">}</span>
        <span class="n">cluster_labels_alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">label_mapping</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">cluster_labels_numeric</span><span class="p">])</span>

        <span class="c1"># --- Gabungkan Data Cluster dengan Fitur Lainnya ---</span>
        <span class="n">df_final_dataset</span> <span class="o">=</span> <span class="n">df_original_full</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># Hapus kolom &#39;sepal_length&#39; yang asli</span>
        <span class="n">df_final_dataset</span> <span class="o">=</span> <span class="n">df_final_dataset</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sepal_length&#39;</span><span class="p">])</span>

        <span class="c1"># Tambahkan kolom klaster baru sebagai pengganti &#39;sepal_length&#39;</span>
        <span class="n">df_final_dataset</span><span class="p">[</span><span class="s1">&#39;sepal_length_cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_labels_alpha</span>

        <span class="c1"># --- Tampilkan DataFrame Hasil Akhir ---</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Berikut adalah DataFrame final Anda dengan kolom klaster sebagai pengganti &#39;sepal_length&#39;:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span><span class="n">df_final_dataset</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s1">&#39;keys&#39;</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s1">&#39;psql&#39;</span><span class="p">,</span> <span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Terjadi Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">finally</span><span class="p">:</span>
    <span class="c1"># Menutup koneksi ke database</span>
    <span class="k">if</span> <span class="s1">&#39;cur&#39;</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">()</span> <span class="ow">and</span> <span class="n">cur</span><span class="p">:</span>
        <span class="n">cur</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">if</span> <span class="s1">&#39;conn&#39;</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">()</span> <span class="ow">and</span> <span class="n">conn</span><span class="p">:</span>
        <span class="n">conn</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Berikut adalah DataFrame final Anda dengan kolom klaster sebagai pengganti &#39;sepal_length&#39;:
+------+---------------+----------------+---------------+------------------------+
|   id |   sepal_width |   petal_length |   petal_width | sepal_length_cluster   |
|------+---------------+----------------+---------------+------------------------|
|    1 |           3.5 |            1.4 |           0.2 | C                      |
|    2 |           3   |            1.4 |           0.2 | C                      |
|    3 |           3.2 |            1.3 |           0.2 | C                      |
|    4 |           3.1 |            1.5 |           0.2 | C                      |
|    5 |           3.6 |            1.4 |           0.2 | C                      |
|    6 |           3.9 |            1.7 |           0.4 | A                      |
|    7 |           3.4 |            1.4 |           0.3 | C                      |
|    8 |           3.4 |            1.5 |           0.2 | C                      |
|    9 |           2.9 |            1.4 |           0.2 | C                      |
|   10 |           3.1 |            1.5 |           0.1 | C                      |
|   11 |           3.7 |            1.5 |           0.2 | A                      |
|   12 |           3.4 |            1.6 |           0.2 | C                      |
|   13 |           3   |            1.4 |           0.1 | C                      |
|   14 |           3   |            1.1 |           0.1 | C                      |
|   15 |           4   |            1.2 |           0.2 | A                      |
|   16 |           4.4 |            1.5 |           0.4 | A                      |
|   17 |           3.9 |            1.3 |           0.4 | A                      |
|   18 |           3.5 |            1.4 |           0.3 | C                      |
|   19 |           3.8 |            1.7 |           0.3 | A                      |
|   20 |           3.8 |            1.5 |           0.3 | C                      |
|   21 |           3.4 |            1.7 |           0.2 | A                      |
|   22 |           3.7 |            1.5 |           0.4 | C                      |
|   23 |           3.6 |            1   |           0.2 | C                      |
|   24 |           3.3 |            1.7 |           0.5 | C                      |
|   25 |           3.4 |            1.9 |           0.2 | C                      |
|   26 |           3   |            1.6 |           0.2 | C                      |
|   27 |           3.4 |            1.6 |           0.4 | C                      |
|   28 |           3.5 |            1.5 |           0.2 | C                      |
|   29 |           3.4 |            1.4 |           0.2 | C                      |
|   30 |           3.2 |            1.6 |           0.2 | C                      |
|   31 |           3.1 |            1.6 |           0.2 | C                      |
|   32 |           3.4 |            1.5 |           0.4 | A                      |
|   33 |           4.1 |            1.5 |           0.1 | C                      |
|   34 |           4.2 |            1.4 |           0.2 | A                      |
|   35 |           3.1 |            1.5 |           0.1 | C                      |
|   36 |           3.2 |            1.2 |           0.2 | C                      |
|   37 |           3.5 |            1.3 |           0.2 | A                      |
|   38 |           3.1 |            1.5 |           0.1 | C                      |
|   39 |           3   |            1.3 |           0.2 | C                      |
|   40 |           3.4 |            1.5 |           0.2 | C                      |
|   41 |           3.5 |            1.3 |           0.3 | C                      |
|   42 |           2.3 |            1.3 |           0.3 | C                      |
|   43 |           3.2 |            1.3 |           0.2 | C                      |
|   44 |           3.5 |            1.6 |           0.6 | C                      |
|   45 |           3.8 |            1.9 |           0.4 | C                      |
|   46 |           3   |            1.4 |           0.3 | C                      |
|   47 |           3.8 |            1.6 |           0.2 | C                      |
|   48 |           3.2 |            1.4 |           0.2 | C                      |
|   49 |           3.7 |            1.5 |           0.2 | C                      |
|   50 |           3.3 |            1.4 |           0.2 | C                      |
|   51 |           3.2 |            4.7 |           1.4 | D                      |
|   52 |           3.2 |            4.5 |           1.5 | D                      |
|   53 |           3.1 |            4.9 |           1.5 | D                      |
|   54 |           2.3 |            4   |           1.3 | A                      |
|   55 |           2.8 |            4.6 |           1.5 | D                      |
|   56 |           2.8 |            4.5 |           1.3 | A                      |
|   57 |           3.3 |            4.7 |           1.6 | D                      |
|   58 |           2.4 |            3.3 |           1   | C                      |
|   59 |           2.9 |            4.6 |           1.3 | D                      |
|   60 |           2.7 |            3.9 |           1.4 | C                      |
|   61 |           2   |            3.5 |           1   | C                      |
|   62 |           3   |            4.2 |           1.5 | A                      |
|   63 |           2.2 |            4   |           1   | A                      |
|   64 |           2.9 |            4.7 |           1.4 | A                      |
|   65 |           2.9 |            3.6 |           1.3 | A                      |
|   66 |           3.1 |            4.4 |           1.4 | D                      |
|   67 |           3   |            4.5 |           1.5 | A                      |
|   68 |           2.7 |            4.1 |           1   | A                      |
|   69 |           2.2 |            4.5 |           1.5 | D                      |
|   70 |           2.5 |            3.9 |           1.1 | A                      |
|   71 |           3.2 |            4.8 |           1.8 | A                      |
|   72 |           2.8 |            4   |           1.3 | A                      |
|   73 |           2.5 |            4.9 |           1.5 | D                      |
|   74 |           2.8 |            4.7 |           1.2 | A                      |
|   75 |           2.9 |            4.3 |           1.3 | D                      |
|   76 |           3   |            4.4 |           1.4 | D                      |
|   77 |           2.8 |            4.8 |           1.4 | D                      |
|   78 |           3   |            5   |           1.7 | D                      |
|   79 |           2.9 |            4.5 |           1.5 | A                      |
|   80 |           2.6 |            3.5 |           1   | A                      |
|   81 |           2.4 |            3.8 |           1.1 | A                      |
|   82 |           2.4 |            3.7 |           1   | A                      |
|   83 |           2.7 |            3.9 |           1.2 | A                      |
|   84 |           2.7 |            5.1 |           1.6 | A                      |
|   85 |           3   |            4.5 |           1.5 | A                      |
|   86 |           3.4 |            4.5 |           1.6 | A                      |
|   87 |           3.1 |            4.7 |           1.5 | D                      |
|   88 |           2.3 |            4.4 |           1.3 | D                      |
|   89 |           3   |            4.1 |           1.3 | A                      |
|   90 |           2.5 |            4   |           1.3 | A                      |
|   91 |           2.6 |            4.4 |           1.2 | A                      |
|   92 |           3   |            4.6 |           1.4 | A                      |
|   93 |           2.6 |            4   |           1.2 | A                      |
|   94 |           2.3 |            3.3 |           1   | C                      |
|   95 |           2.7 |            4.2 |           1.3 | A                      |
|   96 |           3   |            4.2 |           1.2 | A                      |
|   97 |           2.9 |            4.2 |           1.3 | A                      |
|   98 |           2.9 |            4.3 |           1.3 | D                      |
|   99 |           2.5 |            3   |           1.1 | C                      |
|  100 |           2.8 |            4.1 |           1.3 | A                      |
|  101 |           3.3 |            6   |           2.5 | D                      |
|  102 |           2.7 |            5.1 |           1.9 | A                      |
|  103 |           3   |            5.9 |           2.1 | B                      |
|  104 |           2.9 |            5.6 |           1.8 | D                      |
|  105 |           3   |            5.8 |           2.2 | D                      |
|  106 |           3   |            6.6 |           2.1 | B                      |
|  107 |           2.5 |            4.5 |           1.7 | C                      |
|  108 |           2.9 |            6.3 |           1.8 | B                      |
|  109 |           2.5 |            5.8 |           1.8 | D                      |
|  110 |           3.6 |            6.1 |           2.5 | B                      |
|  111 |           3.2 |            5.1 |           2   | D                      |
|  112 |           2.7 |            5.3 |           1.9 | D                      |
|  113 |           3   |            5.5 |           2.1 | D                      |
|  114 |           2.5 |            5   |           2   | A                      |
|  115 |           2.8 |            5.1 |           2.4 | A                      |
|  116 |           3.2 |            5.3 |           2.3 | D                      |
|  117 |           3   |            5.5 |           1.8 | D                      |
|  118 |           3.8 |            6.7 |           2.2 | B                      |
|  119 |           2.6 |            6.9 |           2.3 | B                      |
|  120 |           2.2 |            5   |           1.5 | A                      |
|  121 |           3.2 |            5.7 |           2.3 | D                      |
|  122 |           2.8 |            4.9 |           2   | A                      |
|  123 |           2.8 |            6.7 |           2   | B                      |
|  124 |           2.7 |            4.9 |           1.8 | D                      |
|  125 |           3.3 |            5.7 |           2.1 | D                      |
|  126 |           3.2 |            6   |           1.8 | B                      |
|  127 |           2.8 |            4.8 |           1.8 | D                      |
|  128 |           3   |            4.9 |           1.8 | A                      |
|  129 |           2.8 |            5.6 |           2.1 | D                      |
|  130 |           3   |            5.8 |           1.6 | B                      |
|  131 |           2.8 |            6.1 |           1.9 | B                      |
|  132 |           3.8 |            6.4 |           2   | B                      |
|  133 |           2.8 |            5.6 |           2.2 | D                      |
|  134 |           2.8 |            5.1 |           1.5 | D                      |
|  135 |           2.6 |            5.6 |           1.4 | A                      |
|  136 |           3   |            6.1 |           2.3 | B                      |
|  137 |           3.4 |            5.6 |           2.4 | D                      |
|  138 |           3.1 |            5.5 |           1.8 | D                      |
|  139 |           3   |            4.8 |           1.8 | A                      |
|  140 |           3.1 |            5.4 |           2.1 | D                      |
|  141 |           3.1 |            5.6 |           2.4 | D                      |
|  142 |           3.1 |            5.1 |           2.3 | D                      |
|  143 |           2.7 |            5.1 |           1.9 | A                      |
|  144 |           3.2 |            5.9 |           2.3 | D                      |
|  145 |           3.3 |            5.7 |           2.5 | D                      |
|  146 |           3   |            5.2 |           2.3 | D                      |
|  147 |           2.5 |            5   |           1.9 | D                      |
|  148 |           3   |            5.2 |           2   | D                      |
|  149 |           3.4 |            5.4 |           2.3 | D                      |
|  150 |           3   |            5.1 |           1.8 | A                      |
+------+---------------+----------------+---------------+------------------------+
</pre></div>
</div>
</div>
</div>
</section>
<section id="melakukan-klastering-pada-semua-fitur-menggunakan-k-means-dengan-4-klaster-untuk-fitur-sepal-length-dan-petal-length-dan-3-klaster-untuk-sepal-width-dan-petal-width">
<h2>Melakukan klastering pada semua fitur menggunakan K-Means dengan 4 Klaster untuk fitur Sepal Length dan Petal Length dan 3 klaster untuk Sepal Width dan Petal Width<a class="headerlink" href="#melakukan-klastering-pada-semua-fitur-menggunakan-k-means-dengan-4-klaster-untuk-fitur-sepal-length-dan-petal-length-dan-3-klaster-untuk-sepal-width-dan-petal-width" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pymysql</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tabulate</span><span class="w"> </span><span class="kn">import</span> <span class="n">tabulate</span> <span class="c1"># Import tabulate untuk menampilkan tabel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>

<span class="c1"># --- Konfigurasi Koneksi ke Database MySQL ---</span>
<span class="n">DB_HOST</span> <span class="o">=</span> <span class="s2">&quot;mysql-pendata23-175-mysqlpendata23-175.g.aivencloud.com&quot;</span>
<span class="n">DB_PORT</span> <span class="o">=</span> <span class="mi">17974</span>
<span class="n">DB_NAME</span> <span class="o">=</span> <span class="s2">&quot;defaultdb&quot;</span>
<span class="n">DB_USER</span> <span class="o">=</span> <span class="s2">&quot;avnadmin&quot;</span>
<span class="n">DB_PASS</span> <span class="o">=</span> <span class="s2">&quot;AVNS_756YWhIqpe1WzSKsGNx&quot;</span>

<span class="n">df_original_full</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span> <span class="c1"># Inisialisasi DataFrame kosong</span>

<span class="k">try</span><span class="p">:</span>
    <span class="c1"># Membuat koneksi ke MySQL</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">pymysql</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span>
        <span class="n">host</span><span class="o">=</span><span class="n">DB_HOST</span><span class="p">,</span>
        <span class="n">port</span><span class="o">=</span><span class="n">DB_PORT</span><span class="p">,</span>
        <span class="n">user</span><span class="o">=</span><span class="n">DB_USER</span><span class="p">,</span>
        <span class="n">password</span><span class="o">=</span><span class="n">DB_PASS</span><span class="p">,</span>
        <span class="n">database</span><span class="o">=</span><span class="n">DB_NAME</span><span class="p">,</span>
        <span class="n">cursorclass</span><span class="o">=</span><span class="n">pymysql</span><span class="o">.</span><span class="n">cursors</span><span class="o">.</span><span class="n">DictCursor</span> <span class="c1"># Mengembalikan hasil sebagai dictionary</span>
    <span class="p">)</span>
    <span class="n">cur</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span>

    <span class="c1"># Menjalankan query SQL untuk mengambil semua kolom (`SELECT *`)</span>
    <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;SELECT * FROM irisno.Iris ORDER BY Id ASC;&quot;</span>
    <span class="n">cur</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="n">cur</span><span class="o">.</span><span class="n">fetchall</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">rows</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tabel kosong, tidak ada data untuk diproses.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Mengubah hasil query menjadi DataFrame pandas</span>
        <span class="n">df_original_full</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>

        <span class="c1"># Pastikan nama kolom diubah menjadi format tanpa spasi agar mudah diakses di Python</span>
        <span class="c1"># Kolom-kolom ini akan digunakan untuk klastering atau disimpan kembali</span>
        <span class="k">if</span> <span class="s1">&#39;sepal length&#39;</span> <span class="ow">in</span> <span class="n">df_original_full</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="n">df_original_full</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sepal length&#39;</span><span class="p">:</span> <span class="s1">&#39;sepal length&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="s1">&#39;sepal width&#39;</span> <span class="ow">in</span> <span class="n">df_original_full</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="n">df_original_full</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sepal width&#39;</span><span class="p">:</span> <span class="s1">&#39;sepal width&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="s1">&#39;petal length&#39;</span> <span class="ow">in</span> <span class="n">df_original_full</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="n">df_original_full</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;petal length&#39;</span><span class="p">:</span> <span class="s1">&#39;petal length&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="s1">&#39;petal width&#39;</span> <span class="ow">in</span> <span class="n">df_original_full</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="n">df_original_full</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;petal width&#39;</span><span class="p">:</span> <span class="s1">&#39;petal width&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># Jika ada kolom &#39;Class&#39; dari SELECT *, kita akan tetap menanganinya</span>

        <span class="c1"># --- Fungsi Pembantu untuk Klastering dan Penamaan ---</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">apply_kmeans_and_map_labels</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">column_name</span><span class="p">,</span> <span class="n">n_clusters</span><span class="p">,</span> <span class="n">label_start_char</span><span class="o">=</span><span class="s1">&#39;A&#39;</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Melakukan K-Means clustering pada kolom tertentu dan</span>
<span class="sd">            mengkonversi label numerik menjadi huruf.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">val_X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="n">column_name</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
            <span class="n">k_means_model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span>
                <span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">,</span>
                <span class="n">init</span><span class="o">=</span><span class="s1">&#39;k-means++&#39;</span><span class="p">,</span>
                <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span>
                <span class="n">tol</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
                <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;lloyd&#39;</span>
            <span class="p">)</span>
            <span class="n">k_means_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">val_X</span><span class="p">)</span>
            <span class="n">cluster_labels_numeric</span> <span class="o">=</span> <span class="n">k_means_model</span><span class="o">.</span><span class="n">labels_</span>

            <span class="c1"># Buat mapping dari angka ke huruf (misal: 0-&gt;A, 1-&gt;B, dst.)</span>
            <span class="n">label_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="nb">chr</span><span class="p">(</span><span class="nb">ord</span><span class="p">(</span><span class="n">label_start_char</span><span class="p">)</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">)}</span>
            <span class="n">cluster_labels_alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">label_mapping</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">cluster_labels_numeric</span><span class="p">])</span>

            <span class="k">return</span> <span class="n">cluster_labels_alpha</span>

        <span class="c1"># --- 2. Aplikasikan K-Means dan Konversi Label untuk Setiap Fitur ---</span>

        <span class="c1"># Klastering &#39;sepal_length&#39; dengan 4 klaster (A-D)</span>
        <span class="n">df_original_full</span><span class="p">[</span><span class="s1">&#39;sepal length&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">apply_kmeans_and_map_labels</span><span class="p">(</span>
            <span class="n">df_original_full</span><span class="p">,</span> <span class="s1">&#39;sepal length&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;A&#39;</span>
        <span class="p">)</span>

        <span class="c1"># Klastering &#39;petal_length&#39; dengan 4 klaster (A-D)</span>
        <span class="n">df_original_full</span><span class="p">[</span><span class="s1">&#39;petal length&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">apply_kmeans_and_map_labels</span><span class="p">(</span>
            <span class="n">df_original_full</span><span class="p">,</span> <span class="s1">&#39;petal length&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;A&#39;</span>
        <span class="p">)</span>

        <span class="c1"># Klastering &#39;sepal_width&#39; dengan 3 klaster (A-C)</span>
        <span class="n">df_original_full</span><span class="p">[</span><span class="s1">&#39;sepal width&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">apply_kmeans_and_map_labels</span><span class="p">(</span>
            <span class="n">df_original_full</span><span class="p">,</span> <span class="s1">&#39;sepal width&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;A&#39;</span>
        <span class="p">)</span>

        <span class="c1"># Klastering &#39;petal_width&#39; dengan 3 klaster (A-C)</span>
        <span class="n">df_original_full</span><span class="p">[</span><span class="s1">&#39;petal width&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">apply_kmeans_and_map_labels</span><span class="p">(</span>
            <span class="n">df_original_full</span><span class="p">,</span> <span class="s1">&#39;petal width&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;A&#39;</span>
        <span class="p">)</span>

        <span class="c1"># --- 3. Siapkan DataFrame Final ---</span>
        <span class="c1"># Buat DataFrame baru dengan &#39;id&#39;, &#39;Class&#39; (jika ada), dan kolom-kolom klaster baru</span>
        <span class="n">columns_to_include</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Id&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="s1">&#39;Class&#39;</span> <span class="ow">in</span> <span class="n">df_original_full</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span> <span class="c1"># Cek apakah kolom &#39;Class&#39; ada</span>
            <span class="n">columns_to_include</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">)</span>

        <span class="n">columns_to_include</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span>
            <span class="s1">&#39;sepal length&#39;</span><span class="p">,</span>
            <span class="s1">&#39;sepal width&#39;</span><span class="p">,</span>
            <span class="s1">&#39;petal length&#39;</span><span class="p">,</span>
            <span class="s1">&#39;petal width&#39;</span>
        <span class="p">])</span>

        <span class="n">df_final_dataset</span> <span class="o">=</span> <span class="n">df_original_full</span><span class="p">[</span><span class="n">columns_to_include</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>


        <span class="c1"># --- 4. Tampilkan DataFrame Hasil Akhir ---</span>
        <span class="c1"># Menampilkan seluruh DataFrame menggunakan tabulate</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Berikut adalah DataFrame final Anda dengan kolom klaster sebagai pengganti fitur numerik asli:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span><span class="n">df_final_dataset</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s1">&#39;keys&#39;</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s1">&#39;psql&#39;</span><span class="p">,</span> <span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Terjadi Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">finally</span><span class="p">:</span>
    <span class="c1"># Menutup koneksi ke database</span>
    <span class="k">if</span> <span class="s1">&#39;cur&#39;</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">()</span> <span class="ow">and</span> <span class="n">cur</span><span class="p">:</span>
        <span class="n">cur</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">if</span> <span class="s1">&#39;conn&#39;</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">()</span> <span class="ow">and</span> <span class="n">conn</span><span class="p">:</span>
        <span class="n">conn</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Berikut adalah DataFrame final Anda dengan kolom klaster sebagai pengganti fitur numerik asli:
+------+-----------------+----------------+---------------+----------------+---------------+
|   Id | Class           | sepal length   | sepal width   | petal length   | petal width   |
|------+-----------------+----------------+---------------+----------------+---------------|
|    1 | Iris-setosa     | C              | B             | B              | B             |
|    2 | Iris-setosa     | C              | A             | B              | B             |
|    3 | Iris-setosa     | C              | A             | B              | B             |
|    4 | Iris-setosa     | C              | A             | B              | B             |
|    5 | Iris-setosa     | C              | B             | B              | B             |
|    6 | Iris-setosa     | A              | B             | B              | B             |
|    7 | Iris-setosa     | C              | B             | B              | B             |
|    8 | Iris-setosa     | C              | B             | B              | B             |
|    9 | Iris-setosa     | C              | A             | B              | B             |
|   10 | Iris-setosa     | C              | A             | B              | B             |
|   11 | Iris-setosa     | A              | B             | B              | B             |
|   12 | Iris-setosa     | C              | B             | B              | B             |
|   13 | Iris-setosa     | C              | A             | B              | B             |
|   14 | Iris-setosa     | C              | A             | B              | B             |
|   15 | Iris-setosa     | A              | B             | B              | B             |
|   16 | Iris-setosa     | A              | B             | B              | B             |
|   17 | Iris-setosa     | A              | B             | B              | B             |
|   18 | Iris-setosa     | C              | B             | B              | B             |
|   19 | Iris-setosa     | A              | B             | B              | B             |
|   20 | Iris-setosa     | C              | B             | B              | B             |
|   21 | Iris-setosa     | A              | B             | B              | B             |
|   22 | Iris-setosa     | C              | B             | B              | B             |
|   23 | Iris-setosa     | C              | B             | B              | B             |
|   24 | Iris-setosa     | C              | A             | B              | B             |
|   25 | Iris-setosa     | C              | B             | B              | B             |
|   26 | Iris-setosa     | C              | A             | B              | B             |
|   27 | Iris-setosa     | C              | B             | B              | B             |
|   28 | Iris-setosa     | C              | B             | B              | B             |
|   29 | Iris-setosa     | C              | B             | B              | B             |
|   30 | Iris-setosa     | C              | A             | B              | B             |
|   31 | Iris-setosa     | C              | A             | B              | B             |
|   32 | Iris-setosa     | A              | B             | B              | B             |
|   33 | Iris-setosa     | C              | B             | B              | B             |
|   34 | Iris-setosa     | A              | B             | B              | B             |
|   35 | Iris-setosa     | C              | A             | B              | B             |
|   36 | Iris-setosa     | C              | A             | B              | B             |
|   37 | Iris-setosa     | A              | B             | B              | B             |
|   38 | Iris-setosa     | C              | A             | B              | B             |
|   39 | Iris-setosa     | C              | A             | B              | B             |
|   40 | Iris-setosa     | C              | B             | B              | B             |
|   41 | Iris-setosa     | C              | B             | B              | B             |
|   42 | Iris-setosa     | C              | C             | B              | B             |
|   43 | Iris-setosa     | C              | A             | B              | B             |
|   44 | Iris-setosa     | C              | B             | B              | B             |
|   45 | Iris-setosa     | C              | B             | B              | B             |
|   46 | Iris-setosa     | C              | A             | B              | B             |
|   47 | Iris-setosa     | C              | B             | B              | B             |
|   48 | Iris-setosa     | C              | A             | B              | B             |
|   49 | Iris-setosa     | C              | B             | B              | B             |
|   50 | Iris-setosa     | C              | A             | B              | B             |
|   51 | Iris-versicolor | D              | A             | A              | C             |
|   52 | Iris-versicolor | D              | A             | A              | C             |
|   53 | Iris-versicolor | D              | A             | A              | C             |
|   54 | Iris-versicolor | A              | C             | C              | C             |
|   55 | Iris-versicolor | D              | A             | A              | C             |
|   56 | Iris-versicolor | A              | A             | A              | C             |
|   57 | Iris-versicolor | D              | A             | A              | C             |
|   58 | Iris-versicolor | C              | C             | C              | C             |
|   59 | Iris-versicolor | D              | A             | A              | C             |
|   60 | Iris-versicolor | C              | C             | C              | C             |
|   61 | Iris-versicolor | C              | C             | C              | C             |
|   62 | Iris-versicolor | A              | A             | C              | C             |
|   63 | Iris-versicolor | A              | C             | C              | C             |
|   64 | Iris-versicolor | A              | A             | A              | C             |
|   65 | Iris-versicolor | A              | A             | C              | C             |
|   66 | Iris-versicolor | D              | A             | A              | C             |
|   67 | Iris-versicolor | A              | A             | A              | C             |
|   68 | Iris-versicolor | A              | C             | C              | C             |
|   69 | Iris-versicolor | D              | C             | A              | C             |
|   70 | Iris-versicolor | A              | C             | C              | C             |
|   71 | Iris-versicolor | A              | A             | A              | A             |
|   72 | Iris-versicolor | A              | A             | C              | C             |
|   73 | Iris-versicolor | D              | C             | A              | C             |
|   74 | Iris-versicolor | A              | A             | A              | C             |
|   75 | Iris-versicolor | D              | A             | C              | C             |
|   76 | Iris-versicolor | D              | A             | A              | C             |
|   77 | Iris-versicolor | D              | A             | A              | C             |
|   78 | Iris-versicolor | D              | A             | A              | A             |
|   79 | Iris-versicolor | A              | A             | A              | C             |
|   80 | Iris-versicolor | A              | C             | C              | C             |
|   81 | Iris-versicolor | A              | C             | C              | C             |
|   82 | Iris-versicolor | A              | C             | C              | C             |
|   83 | Iris-versicolor | A              | C             | C              | C             |
|   84 | Iris-versicolor | A              | C             | A              | C             |
|   85 | Iris-versicolor | A              | A             | A              | C             |
|   86 | Iris-versicolor | A              | B             | A              | C             |
|   87 | Iris-versicolor | D              | A             | A              | C             |
|   88 | Iris-versicolor | D              | C             | A              | C             |
|   89 | Iris-versicolor | A              | A             | C              | C             |
|   90 | Iris-versicolor | A              | C             | C              | C             |
|   91 | Iris-versicolor | A              | C             | A              | C             |
|   92 | Iris-versicolor | A              | A             | A              | C             |
|   93 | Iris-versicolor | A              | C             | C              | C             |
|   94 | Iris-versicolor | C              | C             | C              | C             |
|   95 | Iris-versicolor | A              | C             | C              | C             |
|   96 | Iris-versicolor | A              | A             | C              | C             |
|   97 | Iris-versicolor | A              | A             | C              | C             |
|   98 | Iris-versicolor | D              | A             | C              | C             |
|   99 | Iris-versicolor | C              | C             | C              | C             |
|  100 | Iris-versicolor | A              | A             | C              | C             |
|  101 | Iris-virginica  | D              | A             | D              | A             |
|  102 | Iris-virginica  | A              | C             | A              | A             |
|  103 | Iris-virginica  | B              | A             | D              | A             |
|  104 | Iris-virginica  | D              | A             | D              | A             |
|  105 | Iris-virginica  | D              | A             | D              | A             |
|  106 | Iris-virginica  | B              | A             | D              | A             |
|  107 | Iris-virginica  | C              | C             | A              | A             |
|  108 | Iris-virginica  | B              | A             | D              | A             |
|  109 | Iris-virginica  | D              | C             | D              | A             |
|  110 | Iris-virginica  | B              | B             | D              | A             |
|  111 | Iris-virginica  | D              | A             | A              | A             |
|  112 | Iris-virginica  | D              | C             | A              | A             |
|  113 | Iris-virginica  | D              | A             | D              | A             |
|  114 | Iris-virginica  | A              | C             | A              | A             |
|  115 | Iris-virginica  | A              | A             | A              | A             |
|  116 | Iris-virginica  | D              | A             | A              | A             |
|  117 | Iris-virginica  | D              | A             | D              | A             |
|  118 | Iris-virginica  | B              | B             | D              | A             |
|  119 | Iris-virginica  | B              | C             | D              | A             |
|  120 | Iris-virginica  | A              | C             | A              | C             |
|  121 | Iris-virginica  | D              | A             | D              | A             |
|  122 | Iris-virginica  | A              | A             | A              | A             |
|  123 | Iris-virginica  | B              | A             | D              | A             |
|  124 | Iris-virginica  | D              | C             | A              | A             |
|  125 | Iris-virginica  | D              | A             | D              | A             |
|  126 | Iris-virginica  | B              | A             | D              | A             |
|  127 | Iris-virginica  | D              | A             | A              | A             |
|  128 | Iris-virginica  | A              | A             | A              | A             |
|  129 | Iris-virginica  | D              | A             | D              | A             |
|  130 | Iris-virginica  | B              | A             | D              | C             |
|  131 | Iris-virginica  | B              | A             | D              | A             |
|  132 | Iris-virginica  | B              | B             | D              | A             |
|  133 | Iris-virginica  | D              | A             | D              | A             |
|  134 | Iris-virginica  | D              | A             | A              | C             |
|  135 | Iris-virginica  | A              | C             | D              | C             |
|  136 | Iris-virginica  | B              | A             | D              | A             |
|  137 | Iris-virginica  | D              | B             | D              | A             |
|  138 | Iris-virginica  | D              | A             | D              | A             |
|  139 | Iris-virginica  | A              | A             | A              | A             |
|  140 | Iris-virginica  | D              | A             | D              | A             |
|  141 | Iris-virginica  | D              | A             | D              | A             |
|  142 | Iris-virginica  | D              | A             | A              | A             |
|  143 | Iris-virginica  | A              | C             | A              | A             |
|  144 | Iris-virginica  | D              | A             | D              | A             |
|  145 | Iris-virginica  | D              | A             | D              | A             |
|  146 | Iris-virginica  | D              | A             | A              | A             |
|  147 | Iris-virginica  | D              | C             | A              | A             |
|  148 | Iris-virginica  | D              | A             | A              | A             |
|  149 | Iris-virginica  | D              | B             | D              | A             |
|  150 | Iris-virginica  | A              | A             | A              | A             |
+------+-----------------+----------------+---------------+----------------+---------------+
</pre></div>
</div>
</div>
</div>
<section id="klasifikasi-naive-bayes-untuk-data-kategorikal">
<h3>🌼 Klasifikasi Naive Bayes untuk Data Kategorikal<a class="headerlink" href="#klasifikasi-naive-bayes-untuk-data-kategorikal" title="Link to this heading">#</a></h3>
<p><strong>Naive Bayes</strong> adalah sebuah algoritma klasifikasi yang efisien dan didasarkan pada <strong>Teorema Bayes</strong>. Algoritma ini disebut “naive” (lugu) karena mengasumsikan bahwa setiap fitur (variabel prediktor) bersifat <strong>independen</strong> atau tidak saling memengaruhi satu sama lain dalam menentukan kelas.</p>
<p>Untuk kasus di mana data Anda sepenuhnya bersifat kategorikal, varian yang paling sesuai untuk digunakan adalah <code class="docutils literal notranslate"><span class="pre">CategoricalNB</span></code>, yang tersedia dalam pustaka (library) machine learning seperti <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>.</p>
</section>
<hr class="docutils" />
<section id="teorema-bayes">
<h3>📘 Teorema Bayes<a class="headerlink" href="#teorema-bayes" title="Link to this heading">#</a></h3>
<p>Fondasi dari algoritma ini adalah Teorema Bayes, yang secara matematis dirumuskan sebagai berikut:</p>
<div class="math notranslate nohighlight">
\[P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}\]</div>
<p>Dimana:</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(P(C|X)\)</span> (Probabilitas Posterior)</strong>: Ini adalah probabilitas dari suatu kelas <strong><span class="math notranslate nohighlight">\(C\)</span></strong> (misalnya, ‘Iris-setosa’), dengan diketahui adanya data observasi <strong><span class="math notranslate nohighlight">\(X\)</span></strong>. Inilah nilai yang ingin kita maksimalkan untuk menentukan kelas dari data baru.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(P(X|C)\)</span> (Likelihood)</strong>: Probabilitas munculnya data observasi <strong><span class="math notranslate nohighlight">\(X\)</span></strong> dengan syarat data tersebut merupakan bagian dari kelas <strong><span class="math notranslate nohighlight">\(C\)</span></strong>. Dalam Naive Bayes, karena asumsi independensi, nilai ini dihitung sebagai perkalian probabilitas dari setiap fitur:
$<span class="math notranslate nohighlight">\(P(X|C) = P(x_1|C) \cdot P(x_2|C) \cdot \ldots \cdot P(x_n|C)\)</span>$</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(P(C)\)</span> (Probabilitas Prior)</strong>: Probabilitas awal (sebelum melihat data) dari sebuah kelas <strong><span class="math notranslate nohighlight">\(C\)</span></strong>. Biasanya dihitung berdasarkan frekuensi kemunculan setiap kelas pada data pelatihan. Misalnya, jika dari 100 data, 40 di antaranya adalah kelas ‘A’, maka <span class="math notranslate nohighlight">\(P(A) = 0.4\)</span>.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(P(X)\)</span> (Evidence)</strong>: Probabilitas dari data observasi <strong><span class="math notranslate nohighlight">\(X\)</span></strong>. Dalam konteks klasifikasi, nilai ini bertindak sebagai faktor normalisasi dan nilainya akan sama untuk semua kelas. Oleh karena itu, perhitungan <span class="math notranslate nohighlight">\(P(X)\)</span> seringkali <strong>diabaikan</strong> karena tidak memengaruhi perbandingan nilai probabilitas posterior antar kelas.</p></li>
</ul>
</section>
<section id="inisialisa-library">
<h3>Inisialisa Library<a class="headerlink" href="#inisialisa-library" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">CategoricalNB</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrdinalEncoder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="inisialisasi-data-fitur-kategorical">
<h3>Inisialisasi data fitur kategorical<a class="headerlink" href="#inisialisasi-data-fitur-kategorical" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># df_final_dataset = hasil dari proses clustering Anda</span>
<span class="c1"># Kolom: Id, Class, sepal length, sepal width, petal length, petal width</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_final_dataset</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">150</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      Id           Class sepal length sepal width petal length petal width
0      1     Iris-setosa            C           B            B           B
1      2     Iris-setosa            C           A            B           B
2      3     Iris-setosa            C           A            B           B
3      4     Iris-setosa            C           A            B           B
4      5     Iris-setosa            C           B            B           B
..   ...             ...          ...         ...          ...         ...
145  146  Iris-virginica            D           A            A           A
146  147  Iris-virginica            D           C            A           A
147  148  Iris-virginica            D           A            A           A
148  149  Iris-virginica            D           B            D           A
149  150  Iris-virginica            A           A            A           A

[150 rows x 6 columns]
</pre></div>
</div>
</div>
</div>
</section>
<section id="ubah-data-huruf-menjadi-angka-0-3-untuk-4-klaster-dan-0-2-untuk-3-klaster">
<h3>ubah data Huruf menjadi angka 0-3 untuk 4 klaster dan 0-2 untuk 3 klaster<a class="headerlink" href="#ubah-data-huruf-menjadi-angka-0-3-untuk-4-klaster-dan-0-2-untuk-3-klaster" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Memisahkan Fitur X Dan Y yang merupakan Target</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_final_dataset</span><span class="p">[[</span><span class="s1">&#39;sepal length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal width&#39;</span><span class="p">,</span> <span class="s1">&#39;petal length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal width&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_final_dataset</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span>
<span class="c1"># Ubah Fitur Kategorikal ke Numerik (OrdinalEncoder)</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
<span class="n">X_encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>


<span class="c1"># Konversi ke DataFrame agar bisa dilihat dengan nama kolom</span>
<span class="n">df_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_encoded</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Tampilkan 10 data teratas</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Data setelah di-encode (ordinal):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_encoded</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">150</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data setelah di-encode (ordinal):
     sepal length  sepal width  petal length  petal width
0             2.0          1.0           1.0          1.0
1             2.0          0.0           1.0          1.0
2             2.0          0.0           1.0          1.0
3             2.0          0.0           1.0          1.0
4             2.0          1.0           1.0          1.0
..            ...          ...           ...          ...
145           3.0          0.0           0.0          0.0
146           3.0          2.0           0.0          0.0
147           3.0          0.0           0.0          0.0
148           3.0          1.0           3.0          0.0
149           0.0          0.0           0.0          0.0

[150 rows x 4 columns]
</pre></div>
</div>
</div>
</div>
</section>
<section id="latih-data-dan-prediksi-menggunakan-naive-bayes">
<h3>Latih data dan Prediksi menggunakan Naive Bayes<a class="headerlink" href="#latih-data-dan-prediksi-menggunakan-naive-bayes" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrdinalEncoder</span><span class="p">,</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">CategoricalNB</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tabulate</span><span class="w"> </span><span class="kn">import</span> <span class="n">tabulate</span>

<span class="c1"># --- Siapkan Data ---</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_final_dataset</span><span class="p">[[</span><span class="s1">&#39;sepal length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal width&#39;</span><span class="p">,</span> <span class="s1">&#39;petal length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal width&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_final_dataset</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span>

<span class="c1"># --- Encoding kategori fitur ---</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
<span class="n">X_encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># --- Encoding label kelas menjadi angka ---</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y_encoded</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># --- Split data ---</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_encoded</span><span class="p">,</span> <span class="n">y_encoded</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># --- Latih model ---</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CategoricalNB</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># --- Prediksi seluruh data ---</span>
<span class="n">y_pred_all</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_encoded</span><span class="p">)</span>

<span class="c1"># --- Tambahkan hasil prediksi numerik ke DataFrame ---</span>
<span class="n">df_final_dataset_with_pred</span> <span class="o">=</span> <span class="n">df_final_dataset</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">df_final_dataset_with_pred</span><span class="p">[</span><span class="s1">&#39;Prediksi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_all</span>  <span class="c1"># Sudah berupa angka 0, 1, 2</span>

<span class="c1"># --- Tampilkan seluruh data ---</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Seluruh Data Iris dengan Hasil Klasifikasi Naive Bayes (dalam angka) ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span><span class="n">df_final_dataset_with_pred</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s1">&#39;keys&#39;</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s1">&#39;psql&#39;</span><span class="p">,</span> <span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=== Seluruh Data Iris dengan Hasil Klasifikasi Naive Bayes (dalam angka) ===
+------+-----------------+----------------+---------------+----------------+---------------+------------+
|   Id | Class           | sepal length   | sepal width   | petal length   | petal width   |   Prediksi |
|------+-----------------+----------------+---------------+----------------+---------------+------------|
|    1 | Iris-setosa     | C              | B             | B              | B             |          0 |
|    2 | Iris-setosa     | C              | A             | B              | B             |          0 |
|    3 | Iris-setosa     | C              | A             | B              | B             |          0 |
|    4 | Iris-setosa     | C              | A             | B              | B             |          0 |
|    5 | Iris-setosa     | C              | B             | B              | B             |          0 |
|    6 | Iris-setosa     | A              | B             | B              | B             |          0 |
|    7 | Iris-setosa     | C              | B             | B              | B             |          0 |
|    8 | Iris-setosa     | C              | B             | B              | B             |          0 |
|    9 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   10 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   11 | Iris-setosa     | A              | B             | B              | B             |          0 |
|   12 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   13 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   14 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   15 | Iris-setosa     | A              | B             | B              | B             |          0 |
|   16 | Iris-setosa     | A              | B             | B              | B             |          0 |
|   17 | Iris-setosa     | A              | B             | B              | B             |          0 |
|   18 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   19 | Iris-setosa     | A              | B             | B              | B             |          0 |
|   20 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   21 | Iris-setosa     | A              | B             | B              | B             |          0 |
|   22 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   23 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   24 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   25 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   26 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   27 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   28 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   29 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   30 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   31 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   32 | Iris-setosa     | A              | B             | B              | B             |          0 |
|   33 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   34 | Iris-setosa     | A              | B             | B              | B             |          0 |
|   35 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   36 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   37 | Iris-setosa     | A              | B             | B              | B             |          0 |
|   38 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   39 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   40 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   41 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   42 | Iris-setosa     | C              | C             | B              | B             |          0 |
|   43 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   44 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   45 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   46 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   47 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   48 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   49 | Iris-setosa     | C              | B             | B              | B             |          0 |
|   50 | Iris-setosa     | C              | A             | B              | B             |          0 |
|   51 | Iris-versicolor | D              | A             | A              | C             |          1 |
|   52 | Iris-versicolor | D              | A             | A              | C             |          1 |
|   53 | Iris-versicolor | D              | A             | A              | C             |          1 |
|   54 | Iris-versicolor | A              | C             | C              | C             |          1 |
|   55 | Iris-versicolor | D              | A             | A              | C             |          1 |
|   56 | Iris-versicolor | A              | A             | A              | C             |          1 |
|   57 | Iris-versicolor | D              | A             | A              | C             |          1 |
|   58 | Iris-versicolor | C              | C             | C              | C             |          1 |
|   59 | Iris-versicolor | D              | A             | A              | C             |          1 |
|   60 | Iris-versicolor | C              | C             | C              | C             |          1 |
|   61 | Iris-versicolor | C              | C             | C              | C             |          1 |
|   62 | Iris-versicolor | A              | A             | C              | C             |          1 |
|   63 | Iris-versicolor | A              | C             | C              | C             |          1 |
|   64 | Iris-versicolor | A              | A             | A              | C             |          1 |
|   65 | Iris-versicolor | A              | A             | C              | C             |          1 |
|   66 | Iris-versicolor | D              | A             | A              | C             |          1 |
|   67 | Iris-versicolor | A              | A             | A              | C             |          1 |
|   68 | Iris-versicolor | A              | C             | C              | C             |          1 |
|   69 | Iris-versicolor | D              | C             | A              | C             |          1 |
|   70 | Iris-versicolor | A              | C             | C              | C             |          1 |
|   71 | Iris-versicolor | A              | A             | A              | A             |          2 |
|   72 | Iris-versicolor | A              | A             | C              | C             |          1 |
|   73 | Iris-versicolor | D              | C             | A              | C             |          1 |
|   74 | Iris-versicolor | A              | A             | A              | C             |          1 |
|   75 | Iris-versicolor | D              | A             | C              | C             |          1 |
|   76 | Iris-versicolor | D              | A             | A              | C             |          1 |
|   77 | Iris-versicolor | D              | A             | A              | C             |          1 |
|   78 | Iris-versicolor | D              | A             | A              | A             |          2 |
|   79 | Iris-versicolor | A              | A             | A              | C             |          1 |
|   80 | Iris-versicolor | A              | C             | C              | C             |          1 |
|   81 | Iris-versicolor | A              | C             | C              | C             |          1 |
|   82 | Iris-versicolor | A              | C             | C              | C             |          1 |
|   83 | Iris-versicolor | A              | C             | C              | C             |          1 |
|   84 | Iris-versicolor | A              | C             | A              | C             |          1 |
|   85 | Iris-versicolor | A              | A             | A              | C             |          1 |
|   86 | Iris-versicolor | A              | B             | A              | C             |          1 |
|   87 | Iris-versicolor | D              | A             | A              | C             |          1 |
|   88 | Iris-versicolor | D              | C             | A              | C             |          1 |
|   89 | Iris-versicolor | A              | A             | C              | C             |          1 |
|   90 | Iris-versicolor | A              | C             | C              | C             |          1 |
|   91 | Iris-versicolor | A              | C             | A              | C             |          1 |
|   92 | Iris-versicolor | A              | A             | A              | C             |          1 |
|   93 | Iris-versicolor | A              | C             | C              | C             |          1 |
|   94 | Iris-versicolor | C              | C             | C              | C             |          1 |
|   95 | Iris-versicolor | A              | C             | C              | C             |          1 |
|   96 | Iris-versicolor | A              | A             | C              | C             |          1 |
|   97 | Iris-versicolor | A              | A             | C              | C             |          1 |
|   98 | Iris-versicolor | D              | A             | C              | C             |          1 |
|   99 | Iris-versicolor | C              | C             | C              | C             |          1 |
|  100 | Iris-versicolor | A              | A             | C              | C             |          1 |
|  101 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  102 | Iris-virginica  | A              | C             | A              | A             |          2 |
|  103 | Iris-virginica  | B              | A             | D              | A             |          2 |
|  104 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  105 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  106 | Iris-virginica  | B              | A             | D              | A             |          2 |
|  107 | Iris-virginica  | C              | C             | A              | A             |          2 |
|  108 | Iris-virginica  | B              | A             | D              | A             |          2 |
|  109 | Iris-virginica  | D              | C             | D              | A             |          2 |
|  110 | Iris-virginica  | B              | B             | D              | A             |          2 |
|  111 | Iris-virginica  | D              | A             | A              | A             |          2 |
|  112 | Iris-virginica  | D              | C             | A              | A             |          2 |
|  113 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  114 | Iris-virginica  | A              | C             | A              | A             |          2 |
|  115 | Iris-virginica  | A              | A             | A              | A             |          2 |
|  116 | Iris-virginica  | D              | A             | A              | A             |          2 |
|  117 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  118 | Iris-virginica  | B              | B             | D              | A             |          2 |
|  119 | Iris-virginica  | B              | C             | D              | A             |          2 |
|  120 | Iris-virginica  | A              | C             | A              | C             |          1 |
|  121 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  122 | Iris-virginica  | A              | A             | A              | A             |          2 |
|  123 | Iris-virginica  | B              | A             | D              | A             |          2 |
|  124 | Iris-virginica  | D              | C             | A              | A             |          2 |
|  125 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  126 | Iris-virginica  | B              | A             | D              | A             |          2 |
|  127 | Iris-virginica  | D              | A             | A              | A             |          2 |
|  128 | Iris-virginica  | A              | A             | A              | A             |          2 |
|  129 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  130 | Iris-virginica  | B              | A             | D              | C             |          2 |
|  131 | Iris-virginica  | B              | A             | D              | A             |          2 |
|  132 | Iris-virginica  | B              | B             | D              | A             |          2 |
|  133 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  134 | Iris-virginica  | D              | A             | A              | C             |          1 |
|  135 | Iris-virginica  | A              | C             | D              | C             |          1 |
|  136 | Iris-virginica  | B              | A             | D              | A             |          2 |
|  137 | Iris-virginica  | D              | B             | D              | A             |          2 |
|  138 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  139 | Iris-virginica  | A              | A             | A              | A             |          2 |
|  140 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  141 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  142 | Iris-virginica  | D              | A             | A              | A             |          2 |
|  143 | Iris-virginica  | A              | C             | A              | A             |          2 |
|  144 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  145 | Iris-virginica  | D              | A             | D              | A             |          2 |
|  146 | Iris-virginica  | D              | A             | A              | A             |          2 |
|  147 | Iris-virginica  | D              | C             | A              | A             |          2 |
|  148 | Iris-virginica  | D              | A             | A              | A             |          2 |
|  149 | Iris-virginica  | D              | B             | D              | A             |          2 |
|  150 | Iris-virginica  | A              | A             | A              | A             |          2 |
+------+-----------------+----------------+---------------+----------------+---------------+------------+
</pre></div>
</div>
</div>
</div>
</section>
<section id="hitung-akurasinya">
<h3>Hitung Akurasinya<a class="headerlink" href="#hitung-akurasinya" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">accuracy_score(y_true,</span> <span class="pre">y_pred)</span></code> menghitung akurasi, yang didefinisikan sebagai:</p>
<div class="math notranslate nohighlight">
\[
\text{Akurasi} = \frac{\text{Jumlah prediksi benar}}{\text{Jumlah total data uji}}
\]</div>
</section>
<section id="menggunakan-fitur-numerik">
<h3>menggunakan fitur numerik<a class="headerlink" href="#menggunakan-fitur-numerik" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pymysql</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianNB</span>

<span class="c1"># Konfigurasi koneksi ke database MySQL</span>
<span class="n">DB_HOST</span> <span class="o">=</span> <span class="s2">&quot;mysql-pendata23-175-mysqlpendata23-175.g.aivencloud.com&quot;</span>
<span class="n">DB_PORT</span> <span class="o">=</span> <span class="mi">17974</span>
<span class="n">DB_NAME</span> <span class="o">=</span> <span class="s2">&quot;defaultdb&quot;</span>
<span class="n">DB_USER</span> <span class="o">=</span> <span class="s2">&quot;avnadmin&quot;</span>
<span class="n">DB_PASS</span> <span class="o">=</span> <span class="s2">&quot;AVNS_756YWhIqpe1WzSKsGNx&quot;</span>

<span class="k">try</span><span class="p">:</span>
    <span class="c1"># Koneksi ke database</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">pymysql</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span>
        <span class="n">host</span><span class="o">=</span><span class="n">DB_HOST</span><span class="p">,</span>
        <span class="n">port</span><span class="o">=</span><span class="n">DB_PORT</span><span class="p">,</span>
        <span class="n">user</span><span class="o">=</span><span class="n">DB_USER</span><span class="p">,</span>
        <span class="n">password</span><span class="o">=</span><span class="n">DB_PASS</span><span class="p">,</span>
        <span class="n">database</span><span class="o">=</span><span class="n">DB_NAME</span><span class="p">,</span>
        <span class="n">cursorclass</span><span class="o">=</span><span class="n">pymysql</span><span class="o">.</span><span class="n">cursors</span><span class="o">.</span><span class="n">DictCursor</span>
    <span class="p">)</span>
    <span class="n">cur</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span>

    <span class="c1"># Ambil data</span>
    <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;SELECT * FROM irisno.Iris ORDER BY id ASC;&quot;</span>
    <span class="n">cur</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cur</span><span class="o">.</span><span class="n">fetchall</span><span class="p">())</span>

    <span class="c1"># Siapkan data</span>
    <span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span>
    <span class="n">fitur</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;sepal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal_width&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_width&#39;</span><span class="p">]</span>
    <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Class&#39;</span>

    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">fitur</span> <span class="o">+</span> <span class="p">[</span><span class="n">label</span><span class="p">]):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">fitur</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>

        <span class="c1"># Leave-One-Out Evaluation</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
        <span class="n">benar</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="n">i</span><span class="p">]])</span>
            <span class="k">if</span> <span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="n">benar</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">akurasi</span> <span class="o">=</span> <span class="n">benar</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Akurasi Naive Bayes (Data Numerik): </span><span class="si">{</span><span class="n">akurasi</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Kolom fitur atau label tidak lengkap.&quot;</span><span class="p">)</span>

<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

<span class="k">finally</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">cur</span><span class="p">:</span> <span class="n">cur</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">conn</span><span class="p">:</span> <span class="n">conn</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Akurasi Naive Bayes (Data Numerik): 95.33%
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Encode kolom Class (label asli) agar sama dengan format prediksi (angka)</span>
<span class="n">y_asli_encoded</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_final_dataset</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">])</span>

<span class="c1"># Ambil hasil prediksi dari kolom &#39;Prediksi&#39;</span>
<span class="n">y_prediksi</span> <span class="o">=</span> <span class="n">df_final_dataset_with_pred</span><span class="p">[</span><span class="s1">&#39;Prediksi&#39;</span><span class="p">]</span>

<span class="c1"># Hitung akurasi</span>
<span class="n">akurasi</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_asli_encoded</span><span class="p">,</span> <span class="n">y_prediksi</span><span class="p">)</span>

<span class="c1"># Tampilkan hasil akurasi</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Akurasi Prediksi terhadap Data Asli(kategorical): </span><span class="si">{</span><span class="n">akurasi</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> atau </span><span class="si">{</span><span class="n">akurasi</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Akurasi Prediksi terhadap Data Asli(kategorical): 0.9667 atau 96.67%
</pre></div>
</div>
</div>
</div>
</section>
<section id="kesimpulan-perbandingan-akurasi-dari-klasifikasi-menggunakan-naive-bayes-dengan-fitur-kategorical-dan-numerik">
<h3>Kesimpulan perbandingan akurasi dari klasifikasi menggunakan naive bayes dengan fitur kategorical dan numerik<a class="headerlink" href="#kesimpulan-perbandingan-akurasi-dari-klasifikasi-menggunakan-naive-bayes-dengan-fitur-kategorical-dan-numerik" title="Link to this heading">#</a></h3>
<p>Berdasarkan hasil klasifikasi menggunakan algoritma <strong>Naive Bayes</strong>, berikut adalah kesimpulan dari perbandingan performa antara dua jenis data:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Jenis Data</p></th>
<th class="head"><p>Akurasi Naive Bayes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Kategorikal</strong></p></td>
<td><p><strong>96.67%</strong></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Numerik</strong></p></td>
<td><p><strong>95.33%</strong></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="kesimpulan">
<h3>✅ Kesimpulan:<a class="headerlink" href="#kesimpulan" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Model Naive Bayes menunjukkan performa lebih baik saat menggunakan fitur kategorikal</strong>, dengan <strong>akurasi prediksi sebesar 96.67%</strong>, dibandingkan dengan penggunaan fitur numerik yang memberikan akurasi sebesar <strong>95.33%</strong>.</p></li>
<li><p>Hal ini menunjukkan bahwa proses transformasi fitur numerik menjadi kategorikal (misalnya melalui discretization atau clustering) <strong>dapat membantu meningkatkan kemampuan klasifikasi</strong>, terutama dalam konteks data seperti Iris yang memiliki distribusi yang cukup terpisah antar kelas.</p></li>
<li><p>Meskipun perbedaan akurasi relatif kecil (~1.34%), dalam kasus tertentu <strong>data kategorikal dapat lebih mencerminkan pola klasifikasi</strong>, khususnya untuk algoritma yang berbasis probabilistik seperti Naive Bayes.</p></li>
</ul>
</section>
<section id="step-by-step-klasifikasi-dengan-decision-tree-categorical">
<h3>STEP-BY-STEP KLASIFIKASI DENGAN DECISION TREE (CATEGORICAL)<a class="headerlink" href="#step-by-step-klasifikasi-dengan-decision-tree-categorical" title="Link to this heading">#</a></h3>
<p>Pohon Keputusan(Decision Tree adalah salah satu metode dalam penambangan data yang digunakan untuk klasifikasi dan regresi. Model ini sangat populer karena strukturnya yang intuitif, menyerupai diagram alir atau pohon, sehingga mudah dipahami bahkan oleh orang non-teknis.</p>
<p>Secara sederhana, Decision Tree memetakan keputusan-keputusan yang mungkin diambil beserta konsekuensinya. Setiap simpul (node) dalam pohon merepresentasikan sebuah “tes” terhadap suatu atribut (fitur), setiap cabang (branch) merepresentasikan hasil dari tes tersebut, dan setiap daun (leaf node) merepresentasikan label kelas (keputusan akhir).</p>
<p>Bagaimana Cara Kerjanya?</p>
<p>Algoritma Decision Tree bekerja dengan cara mempartisi atau membagi data secara berulang-ulang berdasarkan fitur/atribut tertentu. Tujuannya adalah untuk membuat sub-grup data yang se-“murni” mungkin. “Murni” di sini berarti semua anggota dalam sub-grup tersebut memiliki kelas atau hasil yang sama.</p>
<p>Proses utamanya adalah:</p>
<ul class="simple">
<li><p>Pilih Atribut Terbaik: Algoritma dimulai dari Simpul Akar (Root Node) yang berisi seluruh dataset. Kemudian, ia mencari atribut mana yang paling baik dalam memisahkan data menjadi kelompok-kelompok yang paling murni. Untuk mengukur “kemurnian” ini, metrik seperti Gini Impurity atau Information Gain (Entropy) digunakan.</p></li>
<li><p>Lakukan Pemisahan (Splitting): Dataset dibagi menjadi beberapa subset berdasarkan nilai dari atribut yang terpilih. Setiap subset ini akan menjadi cabang baru.</p></li>
<li><p>Ulangi Proses: Proses nomor 1 dan 2 diulangi untuk setiap subset (yang kini menjadi simpul baru) sampai salah satu kondisi berikut terpenuhi:</p></li>
</ul>
<ol class="arabic simple">
<li><p>Semua data dalam simpul sudah memiliki kelas yang sama (sudah murni).</p></li>
<li><p>Tidak ada lagi atribut yang bisa digunakan untuk memisahkan data.</p></li>
<li><p>Pohon sudah mencapai kedalaman maksimum yang ditentukan sebelumnya untuk mencegahnya menjadi terlalu rumit.</p></li>
<li><p>Simpul yang tidak bisa dibagi lagi disebut Simpul Daun (Leaf Node) dan berisi keputusan atau prediksi akhir.</p></li>
</ol>
<p>Istilah Penting dalam Decision Tree</p>
<ul class="simple">
<li><p>Simpul Akar (Root Node): Simpul paling atas yang mewakili seluruh populasi atau dataset.</p></li>
<li><p>Simpul Keputusan (Decision Node): Simpul yang memiliki dua atau lebih cabang, merepresentasikan sebuah tes pada suatu atribut.</p></li>
<li><p>Simpul Daun (Leaf Node): Simpul akhir yang tidak bisa dipecah lagi dan menunjukkan hasil/keputusan akhir (label kelas).</p></li>
<li><p>Pemisahan (Splitting): Proses membagi sebuah simpul keputusan menjadi sub-simpul.</p></li>
<li><p>Pemangkasan (Pruning): Proses menghilangkan cabang dari pohon keputusan untuk mengurangi kompleksitas dan mencegah overfitting. Overfitting adalah kondisi di mana model terlalu “menghafal” data latih sehingga performanya buruk saat dihadapkan pada data baru.</p></li>
</ul>
</section>
<section id="tampilkan-data-keseluruhan-yang-sudah-menjadi-kategorical">
<h3>Tampilkan data keseluruhan yang sudah menjadi Kategorical<a class="headerlink" href="#tampilkan-data-keseluruhan-yang-sudah-menjadi-kategorical" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load dan siapkan data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_final_dataset</span><span class="p">[[</span><span class="s1">&#39;sepal length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal width&#39;</span><span class="p">,</span> <span class="s1">&#39;petal length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal width&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_final_dataset</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="mengganti-kolom-huruf-menjadi-angka">
<h3>Mengganti kolom Huruf menjadi Angka<a class="headerlink" href="#mengganti-kolom-huruf-menjadi-angka" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrdinalEncoder</span><span class="p">,</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tabulate</span><span class="w"> </span><span class="kn">import</span> <span class="n">tabulate</span>  <span class="c1"># pastikan sudah di-install: pip install tabulate</span>

<span class="c1"># --- Encode fitur kategorikal (A, B, C, ...) menjadi numerik ---</span>
<span class="n">feature_encoder</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
<span class="n">X_encoded</span> <span class="o">=</span> <span class="n">feature_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># --- Encode label target menjadi numerik (Iris-setosa, dll → 0, 1, 2) ---</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y_encoded</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># --- Gabungkan hasil encoding ke dalam DataFrame baru ---</span>
<span class="n">df_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_encoded</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">df_encoded</span><span class="p">[</span><span class="s1">&#39;Class (encoded)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_encoded</span>
<span class="n">df_encoded</span><span class="p">[</span><span class="s1">&#39;Class (original)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># Menambahkan label asli agar bisa dibandingkan</span>

<span class="c1"># --- Tampilkan seluruh DataFrame ---</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Data Iris Setelah Encoding (Fitur dan Label) ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span><span class="n">df_encoded</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s1">&#39;keys&#39;</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s1">&#39;psql&#39;</span><span class="p">,</span> <span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=== Data Iris Setelah Encoding (Fitur dan Label) ===
+----------------+---------------+----------------+---------------+-------------------+--------------------+
|   sepal length |   sepal width |   petal length |   petal width |   Class (encoded) | Class (original)   |
|----------------+---------------+----------------+---------------+-------------------+--------------------|
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              0 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              0 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              0 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              0 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              0 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              0 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              0 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              0 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              0 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              0 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             2 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             1 |              1 |             1 |                 0 | Iris-setosa        |
|              2 |             0 |              1 |             1 |                 0 | Iris-setosa        |
|              3 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              3 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              3 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              0 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              3 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              0 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              3 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              2 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              3 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              2 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              2 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             0 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              0 |             0 |              2 |             2 |                 1 | Iris-versicolor    |
|              3 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              0 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              0 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              3 |             2 |              0 |             2 |                 1 | Iris-versicolor    |
|              0 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             0 |              0 |             0 |                 1 | Iris-versicolor    |
|              0 |             0 |              2 |             2 |                 1 | Iris-versicolor    |
|              3 |             2 |              0 |             2 |                 1 | Iris-versicolor    |
|              0 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              3 |             0 |              2 |             2 |                 1 | Iris-versicolor    |
|              3 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              3 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              3 |             0 |              0 |             0 |                 1 | Iris-versicolor    |
|              0 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              0 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             2 |              0 |             2 |                 1 | Iris-versicolor    |
|              0 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              0 |             1 |              0 |             2 |                 1 | Iris-versicolor    |
|              3 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              3 |             2 |              0 |             2 |                 1 | Iris-versicolor    |
|              0 |             0 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             2 |              0 |             2 |                 1 | Iris-versicolor    |
|              0 |             0 |              0 |             2 |                 1 | Iris-versicolor    |
|              0 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              2 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             0 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             0 |              2 |             2 |                 1 | Iris-versicolor    |
|              3 |             0 |              2 |             2 |                 1 | Iris-versicolor    |
|              2 |             2 |              2 |             2 |                 1 | Iris-versicolor    |
|              0 |             0 |              2 |             2 |                 1 | Iris-versicolor    |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              0 |             2 |              0 |             0 |                 2 | Iris-virginica     |
|              1 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              1 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              2 |             2 |              0 |             0 |                 2 | Iris-virginica     |
|              1 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             2 |              3 |             0 |                 2 | Iris-virginica     |
|              1 |             1 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              0 |             0 |                 2 | Iris-virginica     |
|              3 |             2 |              0 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              0 |             2 |              0 |             0 |                 2 | Iris-virginica     |
|              0 |             0 |              0 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              0 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              1 |             1 |              3 |             0 |                 2 | Iris-virginica     |
|              1 |             2 |              3 |             0 |                 2 | Iris-virginica     |
|              0 |             2 |              0 |             2 |                 2 | Iris-virginica     |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              0 |             0 |              0 |             0 |                 2 | Iris-virginica     |
|              1 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             2 |              0 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              1 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              0 |             0 |                 2 | Iris-virginica     |
|              0 |             0 |              0 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              1 |             0 |              3 |             2 |                 2 | Iris-virginica     |
|              1 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              1 |             1 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              0 |             2 |                 2 | Iris-virginica     |
|              0 |             2 |              3 |             2 |                 2 | Iris-virginica     |
|              1 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             1 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              0 |             0 |              0 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              0 |             0 |                 2 | Iris-virginica     |
|              0 |             2 |              0 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              3 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              0 |             0 |                 2 | Iris-virginica     |
|              3 |             2 |              0 |             0 |                 2 | Iris-virginica     |
|              3 |             0 |              0 |             0 |                 2 | Iris-virginica     |
|              3 |             1 |              3 |             0 |                 2 | Iris-virginica     |
|              0 |             0 |              0 |             0 |                 2 | Iris-virginica     |
+----------------+---------------+----------------+---------------+-------------------+--------------------+
</pre></div>
</div>
</div>
</div>
</section>
<section id="membagi-data-train-dan-data-test-kemudian-melatih-model-tersebut">
<h3>Membagi data train dan data test kemudian melatih model tersebut<a class="headerlink" href="#membagi-data-train-dan-data-test-kemudian-melatih-model-tersebut" title="Link to this heading">#</a></h3>
<hr class="docutils" />
<section id="langkah-langkah-klasifikasi-decision-tree-fitur-kategorikal-03">
<h4>🌳 <strong>Langkah-Langkah Klasifikasi Decision Tree (Fitur Kategorikal 0–3)</strong><a class="headerlink" href="#langkah-langkah-klasifikasi-decision-tree-fitur-kategorikal-03" title="Link to this heading">#</a></h4>
<hr class="docutils" />
<section id="menentukan-tujuan">
<h5>✅ <strong>1. Menentukan Tujuan</strong><a class="headerlink" href="#menentukan-tujuan" title="Link to this heading">#</a></h5>
<p>Tujuan algoritma <strong>Decision Tree Classifier</strong> adalah membuat pohon keputusan yang dapat memisahkan kelas target dengan <strong>impurity (ketidakteraturan)</strong> paling rendah di tiap percabangan.</p>
</section>
<hr class="docutils" />
<section id="hitung-entropy-dataset-awal-root-node">
<h5>🔢 <strong>2. Hitung Entropy Dataset Awal (Root Node)</strong><a class="headerlink" href="#hitung-entropy-dataset-awal-root-node" title="Link to this heading">#</a></h5>
<p>Entropy awal dihitung dari distribusi kelas pada seluruh data (S):</p>
<div class="math notranslate nohighlight">
\[
\text{Entropy}(S) = - \sum_{i=1}^{n} p_i \cdot \log_2(p_i)
\]</div>
<ul class="simple">
<li><p>$p_i$: proporsi kelas ke-$i$ dalam dataset $S$</p></li>
<li><p>$n$: jumlah kelas target</p></li>
</ul>
<p>🧠 <strong>Makna</strong>: Entropy mengukur seberapa tidak homogen dataset tersebut.</p>
</section>
<hr class="docutils" />
<section id="lakukan-split-pada-tiap-fitur">
<h5>✂️ <strong>3. Lakukan Split pada Tiap Fitur</strong><a class="headerlink" href="#lakukan-split-pada-tiap-fitur" title="Link to this heading">#</a></h5>
<p>Untuk <strong>setiap fitur</strong> (yang sudah dalam bentuk kategorikal: 0, 1, 2, 3):</p>
<ol class="arabic simple">
<li><p>Coba <strong>berbagai kemungkinan split</strong>, misalnya:</p>
<ul class="simple">
<li><p>Split-1: {0} vs {1,2,3}</p></li>
<li><p>Split-2: {0,1} vs {2,3}</p></li>
<li><p>Split-3: {0,1,2} vs {3}</p></li>
</ul>
</li>
<li><p>Hitung Entropy untuk tiap subset hasil split.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="hitung-split-entropy-entropy-setelah-split">
<h5>📘 <strong>4. Hitung Split Entropy (Entropy Setelah Split)</strong><a class="headerlink" href="#hitung-split-entropy-entropy-setelah-split" title="Link to this heading">#</a></h5>
<p>Gunakan rumus <strong>Split Entropy</strong>:</p>
<div class="math notranslate nohighlight">
\[
\text{Entropy}_{\text{split}}(S, A) = \sum_{v \in \text{Groups}} \frac{|S_v|}{|S|} \cdot \text{Entropy}(S_v)
\]</div>
<ul class="simple">
<li><p>$S_v$: subset dari $S$ yang masuk dalam grup tertentu</p></li>
<li><p>$\frac{|S_v|}{|S|}$: proporsi jumlah data subset terhadap total</p></li>
</ul>
<p>🧠 <strong>Makna</strong>: Split entropy mengukur ketidakteraturan rata-rata dari hasil pembagian menggunakan fitur $A$.</p>
</section>
<hr class="docutils" />
<section id="hitung-information-gain">
<h5>🔍 <strong>5. Hitung Information Gain</strong><a class="headerlink" href="#hitung-information-gain" title="Link to this heading">#</a></h5>
<p>Gunakan rumus:</p>
<div class="math notranslate nohighlight">
\[
\text{Information Gain}(S, A) = \text{Entropy}(S) - \text{Entropy}_{\text{split}}(S, A)
\]</div>
<p>🧠 <strong>Makna</strong>: Semakin besar gain, semakin baik fitur tersebut digunakan untuk pemisahan.</p>
</section>
<hr class="docutils" />
<section id="pilih-split-terbaik-threshold">
<h5>🔪 <strong>6. Pilih Split Terbaik (Threshold)</strong><a class="headerlink" href="#pilih-split-terbaik-threshold" title="Link to this heading">#</a></h5>
<p>Untuk fitur kategorikal:</p>
<ul class="simple">
<li><p>Threshold bisa berupa subset nilai yang masuk ke kiri vs kanan:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">{fitur</span> <span class="pre">≤</span> <span class="pre">t}</span></code> jika diasumsikan urut</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">{fitur</span> <span class="pre">∈</span> <span class="pre">[0,2]}</span></code> vs lainnya jika tidak urut</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="buat-node-baru-dan-ulangi-rekursif">
<h5>🌿 <strong>7. Buat Node Baru dan Ulangi Rekursif</strong><a class="headerlink" href="#buat-node-baru-dan-ulangi-rekursif" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p>Buat <strong>node cabang</strong> berdasarkan split terbaik</p></li>
<li><p>Ulangi langkah 2–6 <strong>pada masing-masing subset</strong> hingga:</p>
<ul>
<li><p>Semua data homogen</p></li>
<li><p>Atau tidak ada fitur lagi</p></li>
<li><p>Atau kondisi stopping terpenuhi (misal: max depth)</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="simbol-dan-penjelasan">
<h5>📌 Simbol dan Penjelasan<a class="headerlink" href="#simbol-dan-penjelasan" title="Link to this heading">#</a></h5>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Simbol</p></th>
<th class="head"><p>Makna</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>$S$</p></td>
<td><p>Dataset pada node saat ini</p></td>
</tr>
<tr class="row-odd"><td><p>$A$</p></td>
<td><p>Fitur yang sedang diuji</p></td>
</tr>
<tr class="row-even"><td><p>$S_v$</p></td>
<td><p>Subset dari $S$ berdasarkan nilai tertentu dari $A$</p></td>
</tr>
<tr class="row-odd"><td><p>$p_i$</p></td>
<td><p>Proporsi data kelas ke-$i$</p></td>
</tr>
<tr class="row-even"><td><p>$t$</p></td>
<td><p>Threshold untuk split (nilai ambang)</p></td>
</tr>
<tr class="row-odd"><td><p>$\text{Entropy}(S)$</p></td>
<td><p>Entropy data saat ini</p></td>
</tr>
<tr class="row-even"><td><p>$\text{Entropy}_{\text{split}}(S, A)$</p></td>
<td><p>Entropy setelah split oleh fitur $A$</p></td>
</tr>
<tr class="row-odd"><td><p>$\text{Information Gain}(S, A)$</p></td>
<td><p>Nilai pengurangan impurity karena split</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
</section>
<hr class="docutils" />
<section id="ilustrasi-decision-tree-dengan-fitur-kategorikal-03">
<h2>✅ Ilustrasi: Decision Tree dengan Fitur Kategorikal 0–3<a class="headerlink" href="#ilustrasi-decision-tree-dengan-fitur-kategorikal-03" title="Link to this heading">#</a></h2>
<p>Misal fitur <code class="docutils literal notranslate"><span class="pre">petal_width</span></code> sudah dikategorikan:</p>
<ul class="simple">
<li><p>Split-1: <code class="docutils literal notranslate"><span class="pre">petal_width</span></code> = 0 vs {1,2,3}</p></li>
<li><p>Hitung Entropy di kedua sisi</p></li>
<li><p>Hitung Information Gain</p></li>
<li><p>Pilih split dengan gain tertinggi</p></li>
<li><p>Lanjutkan ke node berikutnya</p></li>
</ul>
<hr class="docutils" />
<p>Jika kamu ingin, saya bisa bantu:</p>
<ul class="simple">
<li><p>👨‍💻 Kode Python menghitung entropy &amp; gain untuk 1 fitur</p></li>
<li><p>📊 Gambar struktur tree dengan logika tersebut</p></li>
<li><p>📥 Simpan model / prediksi satu data manual</p></li>
</ul>
<p>Ingin lanjut ke perhitungan entropy manual atau kode evaluasi akurasi?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_encoded</span><span class="p">,</span> <span class="n">y_encoded</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;entropy&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># Atau &#39;entropy&#39;</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>DecisionTreeClassifier</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html">?<span>Documentation for DecisionTreeClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, random_state=42)</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrdinalEncoder</span><span class="p">,</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tabulate</span><span class="w"> </span><span class="kn">import</span> <span class="n">tabulate</span>

<span class="c1"># --- Ambil fitur &amp; label ---</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_final_dataset</span><span class="p">[[</span><span class="s1">&#39;sepal length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal width&#39;</span><span class="p">,</span> <span class="s1">&#39;petal length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal width&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_final_dataset</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span>

<span class="c1"># --- Encoding fitur dan label menjadi angka ---</span>
<span class="n">feature_encoder</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
<span class="n">X_encoded</span> <span class="o">=</span> <span class="n">feature_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y_encoded</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># --- Split data ---</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_encoded</span><span class="p">,</span> <span class="n">y_encoded</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># --- Latih model Decision Tree ---</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;entropy&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># --- Prediksi seluruh data (bukan hanya X_test) ---</span>
<span class="n">y_pred_all</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_encoded</span><span class="p">)</span>

<span class="c1"># --- Evaluasi hanya pada data uji ---</span>
<span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Confusion Matrix ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Classification Report ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Akurasi Model pada Data Uji: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ===&quot;</span><span class="p">)</span>

<span class="c1"># --- Tambahkan prediksi ke DataFrame utama ---</span>
<span class="n">df_with_prediction</span> <span class="o">=</span> <span class="n">df_final_dataset</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">df_with_prediction</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_encoded</span>  <span class="c1"># ubah label ke angka 0-3</span>
<span class="n">df_with_prediction</span><span class="p">[</span><span class="s1">&#39;Predict&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_all</span>

<span class="c1"># --- Tampilkan seluruh data dengan kolom prediksi ---</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Seluruh Data dengan Hasil Prediksi Decision Tree ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span><span class="n">df_with_prediction</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s1">&#39;keys&#39;</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s1">&#39;psql&#39;</span><span class="p">,</span> <span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=== Confusion Matrix ===
[[19  0  0]
 [ 0 13  0]
 [ 0  0 13]]

=== Classification Report ===
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        13
           2       1.00      1.00      1.00        13

    accuracy                           1.00        45
   macro avg       1.00      1.00      1.00        45
weighted avg       1.00      1.00      1.00        45


=== Akurasi Model pada Data Uji: 1.0000 ===

=== Seluruh Data dengan Hasil Prediksi Decision Tree ===
+------+---------+----------------+---------------+----------------+---------------+-----------+
|   Id |   Class | sepal length   | sepal width   | petal length   | petal width   |   Predict |
|------+---------+----------------+---------------+----------------+---------------+-----------|
|    1 |       0 | C              | B             | B              | B             |         0 |
|    2 |       0 | C              | A             | B              | B             |         0 |
|    3 |       0 | C              | A             | B              | B             |         0 |
|    4 |       0 | C              | A             | B              | B             |         0 |
|    5 |       0 | C              | B             | B              | B             |         0 |
|    6 |       0 | A              | B             | B              | B             |         0 |
|    7 |       0 | C              | B             | B              | B             |         0 |
|    8 |       0 | C              | B             | B              | B             |         0 |
|    9 |       0 | C              | A             | B              | B             |         0 |
|   10 |       0 | C              | A             | B              | B             |         0 |
|   11 |       0 | A              | B             | B              | B             |         0 |
|   12 |       0 | C              | B             | B              | B             |         0 |
|   13 |       0 | C              | A             | B              | B             |         0 |
|   14 |       0 | C              | A             | B              | B             |         0 |
|   15 |       0 | A              | B             | B              | B             |         0 |
|   16 |       0 | A              | B             | B              | B             |         0 |
|   17 |       0 | A              | B             | B              | B             |         0 |
|   18 |       0 | C              | B             | B              | B             |         0 |
|   19 |       0 | A              | B             | B              | B             |         0 |
|   20 |       0 | C              | B             | B              | B             |         0 |
|   21 |       0 | A              | B             | B              | B             |         0 |
|   22 |       0 | C              | B             | B              | B             |         0 |
|   23 |       0 | C              | B             | B              | B             |         0 |
|   24 |       0 | C              | A             | B              | B             |         0 |
|   25 |       0 | C              | B             | B              | B             |         0 |
|   26 |       0 | C              | A             | B              | B             |         0 |
|   27 |       0 | C              | B             | B              | B             |         0 |
|   28 |       0 | C              | B             | B              | B             |         0 |
|   29 |       0 | C              | B             | B              | B             |         0 |
|   30 |       0 | C              | A             | B              | B             |         0 |
|   31 |       0 | C              | A             | B              | B             |         0 |
|   32 |       0 | A              | B             | B              | B             |         0 |
|   33 |       0 | C              | B             | B              | B             |         0 |
|   34 |       0 | A              | B             | B              | B             |         0 |
|   35 |       0 | C              | A             | B              | B             |         0 |
|   36 |       0 | C              | A             | B              | B             |         0 |
|   37 |       0 | A              | B             | B              | B             |         0 |
|   38 |       0 | C              | A             | B              | B             |         0 |
|   39 |       0 | C              | A             | B              | B             |         0 |
|   40 |       0 | C              | B             | B              | B             |         0 |
|   41 |       0 | C              | B             | B              | B             |         0 |
|   42 |       0 | C              | C             | B              | B             |         0 |
|   43 |       0 | C              | A             | B              | B             |         0 |
|   44 |       0 | C              | B             | B              | B             |         0 |
|   45 |       0 | C              | B             | B              | B             |         0 |
|   46 |       0 | C              | A             | B              | B             |         0 |
|   47 |       0 | C              | B             | B              | B             |         0 |
|   48 |       0 | C              | A             | B              | B             |         0 |
|   49 |       0 | C              | B             | B              | B             |         0 |
|   50 |       0 | C              | A             | B              | B             |         0 |
|   51 |       1 | D              | A             | A              | C             |         1 |
|   52 |       1 | D              | A             | A              | C             |         1 |
|   53 |       1 | D              | A             | A              | C             |         1 |
|   54 |       1 | A              | C             | C              | C             |         1 |
|   55 |       1 | D              | A             | A              | C             |         1 |
|   56 |       1 | A              | A             | A              | C             |         1 |
|   57 |       1 | D              | A             | A              | C             |         1 |
|   58 |       1 | C              | C             | C              | C             |         1 |
|   59 |       1 | D              | A             | A              | C             |         1 |
|   60 |       1 | C              | C             | C              | C             |         1 |
|   61 |       1 | C              | C             | C              | C             |         1 |
|   62 |       1 | A              | A             | C              | C             |         1 |
|   63 |       1 | A              | C             | C              | C             |         1 |
|   64 |       1 | A              | A             | A              | C             |         1 |
|   65 |       1 | A              | A             | C              | C             |         1 |
|   66 |       1 | D              | A             | A              | C             |         1 |
|   67 |       1 | A              | A             | A              | C             |         1 |
|   68 |       1 | A              | C             | C              | C             |         1 |
|   69 |       1 | D              | C             | A              | C             |         1 |
|   70 |       1 | A              | C             | C              | C             |         1 |
|   71 |       1 | A              | A             | A              | A             |         2 |
|   72 |       1 | A              | A             | C              | C             |         1 |
|   73 |       1 | D              | C             | A              | C             |         1 |
|   74 |       1 | A              | A             | A              | C             |         1 |
|   75 |       1 | D              | A             | C              | C             |         1 |
|   76 |       1 | D              | A             | A              | C             |         1 |
|   77 |       1 | D              | A             | A              | C             |         1 |
|   78 |       1 | D              | A             | A              | A             |         2 |
|   79 |       1 | A              | A             | A              | C             |         1 |
|   80 |       1 | A              | C             | C              | C             |         1 |
|   81 |       1 | A              | C             | C              | C             |         1 |
|   82 |       1 | A              | C             | C              | C             |         1 |
|   83 |       1 | A              | C             | C              | C             |         1 |
|   84 |       1 | A              | C             | A              | C             |         1 |
|   85 |       1 | A              | A             | A              | C             |         1 |
|   86 |       1 | A              | B             | A              | C             |         1 |
|   87 |       1 | D              | A             | A              | C             |         1 |
|   88 |       1 | D              | C             | A              | C             |         1 |
|   89 |       1 | A              | A             | C              | C             |         1 |
|   90 |       1 | A              | C             | C              | C             |         1 |
|   91 |       1 | A              | C             | A              | C             |         1 |
|   92 |       1 | A              | A             | A              | C             |         1 |
|   93 |       1 | A              | C             | C              | C             |         1 |
|   94 |       1 | C              | C             | C              | C             |         1 |
|   95 |       1 | A              | C             | C              | C             |         1 |
|   96 |       1 | A              | A             | C              | C             |         1 |
|   97 |       1 | A              | A             | C              | C             |         1 |
|   98 |       1 | D              | A             | C              | C             |         1 |
|   99 |       1 | C              | C             | C              | C             |         1 |
|  100 |       1 | A              | A             | C              | C             |         1 |
|  101 |       2 | D              | A             | D              | A             |         2 |
|  102 |       2 | A              | C             | A              | A             |         2 |
|  103 |       2 | B              | A             | D              | A             |         2 |
|  104 |       2 | D              | A             | D              | A             |         2 |
|  105 |       2 | D              | A             | D              | A             |         2 |
|  106 |       2 | B              | A             | D              | A             |         2 |
|  107 |       2 | C              | C             | A              | A             |         2 |
|  108 |       2 | B              | A             | D              | A             |         2 |
|  109 |       2 | D              | C             | D              | A             |         2 |
|  110 |       2 | B              | B             | D              | A             |         2 |
|  111 |       2 | D              | A             | A              | A             |         2 |
|  112 |       2 | D              | C             | A              | A             |         2 |
|  113 |       2 | D              | A             | D              | A             |         2 |
|  114 |       2 | A              | C             | A              | A             |         2 |
|  115 |       2 | A              | A             | A              | A             |         2 |
|  116 |       2 | D              | A             | A              | A             |         2 |
|  117 |       2 | D              | A             | D              | A             |         2 |
|  118 |       2 | B              | B             | D              | A             |         2 |
|  119 |       2 | B              | C             | D              | A             |         2 |
|  120 |       2 | A              | C             | A              | C             |         1 |
|  121 |       2 | D              | A             | D              | A             |         2 |
|  122 |       2 | A              | A             | A              | A             |         2 |
|  123 |       2 | B              | A             | D              | A             |         2 |
|  124 |       2 | D              | C             | A              | A             |         2 |
|  125 |       2 | D              | A             | D              | A             |         2 |
|  126 |       2 | B              | A             | D              | A             |         2 |
|  127 |       2 | D              | A             | A              | A             |         2 |
|  128 |       2 | A              | A             | A              | A             |         2 |
|  129 |       2 | D              | A             | D              | A             |         2 |
|  130 |       2 | B              | A             | D              | C             |         2 |
|  131 |       2 | B              | A             | D              | A             |         2 |
|  132 |       2 | B              | B             | D              | A             |         2 |
|  133 |       2 | D              | A             | D              | A             |         2 |
|  134 |       2 | D              | A             | A              | C             |         1 |
|  135 |       2 | A              | C             | D              | C             |         2 |
|  136 |       2 | B              | A             | D              | A             |         2 |
|  137 |       2 | D              | B             | D              | A             |         2 |
|  138 |       2 | D              | A             | D              | A             |         2 |
|  139 |       2 | A              | A             | A              | A             |         2 |
|  140 |       2 | D              | A             | D              | A             |         2 |
|  141 |       2 | D              | A             | D              | A             |         2 |
|  142 |       2 | D              | A             | A              | A             |         2 |
|  143 |       2 | A              | C             | A              | A             |         2 |
|  144 |       2 | D              | A             | D              | A             |         2 |
|  145 |       2 | D              | A             | D              | A             |         2 |
|  146 |       2 | D              | A             | A              | A             |         2 |
|  147 |       2 | D              | C             | A              | A             |         2 |
|  148 |       2 | D              | A             | A              | A             |         2 |
|  149 |       2 | D              | B             | D              | A             |         2 |
|  150 |       2 | A              | A             | A              | A             |         2 |
+------+---------+----------------+---------------+----------------+---------------+-----------+
</pre></div>
</div>
</div>
</div>
<section id="visualisasi-decision-tree">
<h3>Visualisasi Decision Tree<a class="headerlink" href="#visualisasi-decision-tree" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">plot_tree</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

<span class="n">plot_tree</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">feature_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sepal length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal width&#39;</span><span class="p">,</span> <span class="s1">&#39;petal length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal width&#39;</span><span class="p">],</span>
    <span class="n">class_names</span><span class="o">=</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span>
    <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">impurity</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>         <span class="c1"># Menampilkan impurity (Gini Index)</span>
    <span class="n">proportion</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>       <span class="c1"># Menampilkan proporsi sample</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Visualisasi Lengkap Decision Tree (Iris - Kategorikal)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0c3a8518b0cccfee03f8dd9a5f27e437def0bd24ab30f776818f503eb0f61c5d.png" src="_images/0c3a8518b0cccfee03f8dd9a5f27e437def0bd24ab30f776818f503eb0f61c5d.png" />
</div>
</div>
</section>
<section id="id2">
<h3>Hitung Akurasinya<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">accuracy_score(y_true,</span> <span class="pre">y_pred)</span></code> menghitung akurasi, yang didefinisikan sebagai:</p>
<div class="math notranslate nohighlight">
\[
\text{Akurasi} = \frac{\text{Jumlah prediksi benar}}{\text{Jumlah total data uji}}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Encode label asli (&#39;Class&#39;) agar bisa dibandingkan dengan y_pred_all</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y_encoded_true</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_result</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">])</span>

<span class="c1"># y_pred_all sudah berisi hasil prediksi (angka)</span>
<span class="n">y_pred_all</span> <span class="o">=</span> <span class="n">df_result</span><span class="p">[</span><span class="s1">&#39;Prediksi&#39;</span><span class="p">]</span>

<span class="c1"># Hitung akurasi</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_encoded_true</span><span class="p">,</span> <span class="n">y_pred_all</span><span class="p">)</span>

<span class="c1"># Tampilkan hasil</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Akurasi Klasifikasi pada Seluruh Data(dengan kolom Kategorical): </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> atau </span><span class="si">{</span><span class="n">accuracy</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="nn">/tmp/ipython-input-17-4210538197.py</span> in <span class="ni">&lt;cell line: 0&gt;</span><span class="nt">()</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="ne">----&gt; </span><span class="mi">6</span> <span class="n">y_encoded_true</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_result</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">])</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> 
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="c1"># y_pred_all sudah berisi hasil prediksi (angka)</span>

<span class="ne">NameError</span>: name &#39;df_result&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Pastikan kolom numerik tanpa spasi</span>
<span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span>
<span class="n">numeric_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;sepal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal_width&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_width&#39;</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="n">numeric_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">numeric_cols</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

<span class="c1"># Encode label jika belum dalam format numerik</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">])</span>

<span class="c1"># Fitur dan target</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">numeric_cols</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span>

<span class="c1"># Leave-One-Out Evaluation untuk Decision Tree</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;entropy&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)):</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">y_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;🌳 Akurasi Decision Tree (Fitur numerik): </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> atau </span><span class="si">{</span><span class="n">accuracy</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>🌳 Akurasi Decision Tree (Fitur numerik): 0.9400 atau 94.00%
</pre></div>
</div>
</div>
</div>
</section>
<section id="kesimpulan-perbandingan-akurasi-pada-klasifikasi-data-iris-dengan-menggunakan-decision-tree-antara-data-numerik-dan-kategorical">
<h3>Kesimpulan Perbandingan Akurasi pada klasifikasi data Iris dengan menggunakan Decision Tree antara data numerik dan kategorical<a class="headerlink" href="#kesimpulan-perbandingan-akurasi-pada-klasifikasi-data-iris-dengan-menggunakan-decision-tree-antara-data-numerik-dan-kategorical" title="Link to this heading">#</a></h3>
</section>
<section id="kesimpulan-perbandingan-akurasi-decision-tree-pada-data-iris">
<h3>📊 Kesimpulan Perbandingan Akurasi Decision Tree pada Data Iris<a class="headerlink" href="#kesimpulan-perbandingan-akurasi-decision-tree-pada-data-iris" title="Link to this heading">#</a></h3>
<p>Berikut adalah hasil perbandingan akurasi klasifikasi menggunakan algoritma <strong>Decision Tree</strong> pada data <strong>Iris</strong>, dengan dua jenis representasi fitur:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Jenis Fitur</p></th>
<th class="head"><p>Akurasi Decision Tree</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Kategorikal</strong></p></td>
<td><p><strong>97.33%</strong></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Numerik</strong></p></td>
<td><p><strong>94.00%</strong></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="id3">
<h3>✅ Kesimpulan:<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Model <strong>Decision Tree menghasilkan akurasi lebih tinggi saat menggunakan fitur kategorikal</strong> (97.33%) dibandingkan dengan fitur numerik (94.00%).</p></li>
<li><p>Ini menunjukkan bahwa <strong>representasi kategorikal</strong> dari fitur numerik (misalnya melalui proses discretization atau klasterisasi) dapat <strong>membantu meningkatkan performa klasifikasi</strong>, terutama ketika data memiliki batasan yang jelas antar kelas.</p></li>
<li><p>Decision Tree memang cenderung cocok dengan data kategorikal karena struktur pohonnya bekerja berdasarkan pembelahan nilai yang diskrit.</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="DecisionTree.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Decision Tree</p>
      </div>
    </a>
    <a class="right-next"
       href="klasifikasiResikoKematianIbu.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Klasifikasi Resiko Kesehatan Ibu</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#instalasi-library-yang-dibutuhkan">Instalasi library yang dibutuhkan</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualisasi-data-iris-dengan-menyisahkan-kolom-sepal-length">Visualisasi data Iris dengan menyisahkan kolom sepal Length</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#melakukan-klastering-dengan-menggunakan-k-means-klastering">Melakukan Klastering dengan menggunakan K-Means klastering</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tujuan-dan-fungsi-klustering">tujuan dan fungsi klustering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tujuan-dan-fungsi-klaster-clustering">Tujuan dan Fungsi Klaster (Clustering)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tujuan-klastering">🎯 <strong>Tujuan Klastering</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fungsi-klastering-dalam-analisis-data">🛠️ <strong>Fungsi Klastering dalam Analisis Data</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langkah-langkah-algoritma-k-means">Langkah Langkah Algoritma K-Means</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#penjelasan">Penjelasan:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">✅ Penjelasan:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#menggabungkan-data-yang-sudah-di-klastering-menggunakan-k-means-pada-kolom-sepal-length-dengan-kolom-yang-di-drop-tadi-mengembalikan-nilai-sepal-length-berupa-kategorical">Menggabungkan data yang sudah di klastering menggunakan K-Means pada kolom Sepal Length dengan kolom yang di drop tadi mengembalikan nilai Sepal Length berupa kategorical</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#melakukan-klastering-pada-semua-fitur-menggunakan-k-means-dengan-4-klaster-untuk-fitur-sepal-length-dan-petal-length-dan-3-klaster-untuk-sepal-width-dan-petal-width">Melakukan klastering pada semua fitur menggunakan K-Means dengan 4 Klaster untuk fitur Sepal Length dan Petal Length dan 3 klaster untuk Sepal Width dan Petal Width</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#klasifikasi-naive-bayes-untuk-data-kategorikal">🌼 Klasifikasi Naive Bayes untuk Data Kategorikal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#teorema-bayes">📘 Teorema Bayes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inisialisa-library">Inisialisa Library</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inisialisasi-data-fitur-kategorical">Inisialisasi data fitur kategorical</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ubah-data-huruf-menjadi-angka-0-3-untuk-4-klaster-dan-0-2-untuk-3-klaster">ubah data Huruf menjadi angka 0-3 untuk 4 klaster dan 0-2 untuk 3 klaster</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latih-data-dan-prediksi-menggunakan-naive-bayes">Latih data dan Prediksi menggunakan Naive Bayes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hitung-akurasinya">Hitung Akurasinya</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#menggunakan-fitur-numerik">menggunakan fitur numerik</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kesimpulan-perbandingan-akurasi-dari-klasifikasi-menggunakan-naive-bayes-dengan-fitur-kategorical-dan-numerik">Kesimpulan perbandingan akurasi dari klasifikasi menggunakan naive bayes dengan fitur kategorical dan numerik</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kesimpulan">✅ Kesimpulan:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-klasifikasi-dengan-decision-tree-categorical">STEP-BY-STEP KLASIFIKASI DENGAN DECISION TREE (CATEGORICAL)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tampilkan-data-keseluruhan-yang-sudah-menjadi-kategorical">Tampilkan data keseluruhan yang sudah menjadi Kategorical</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mengganti-kolom-huruf-menjadi-angka">Mengganti kolom Huruf menjadi Angka</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#membagi-data-train-dan-data-test-kemudian-melatih-model-tersebut">Membagi data train dan data test kemudian melatih model tersebut</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#langkah-langkah-klasifikasi-decision-tree-fitur-kategorikal-03">🌳 <strong>Langkah-Langkah Klasifikasi Decision Tree (Fitur Kategorikal 0–3)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#menentukan-tujuan">✅ <strong>1. Menentukan Tujuan</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#hitung-entropy-dataset-awal-root-node">🔢 <strong>2. Hitung Entropy Dataset Awal (Root Node)</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#lakukan-split-pada-tiap-fitur">✂️ <strong>3. Lakukan Split pada Tiap Fitur</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#hitung-split-entropy-entropy-setelah-split">📘 <strong>4. Hitung Split Entropy (Entropy Setelah Split)</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#hitung-information-gain">🔍 <strong>5. Hitung Information Gain</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#pilih-split-terbaik-threshold">🔪 <strong>6. Pilih Split Terbaik (Threshold)</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#buat-node-baru-dan-ulangi-rekursif">🌿 <strong>7. Buat Node Baru dan Ulangi Rekursif</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#simbol-dan-penjelasan">📌 Simbol dan Penjelasan</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ilustrasi-decision-tree-dengan-fitur-kategorikal-03">✅ Ilustrasi: Decision Tree dengan Fitur Kategorikal 0–3</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualisasi-decision-tree">Visualisasi Decision Tree</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Hitung Akurasinya</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kesimpulan-perbandingan-akurasi-pada-klasifikasi-data-iris-dengan-menggunakan-decision-tree-antara-data-numerik-dan-kategorical">Kesimpulan Perbandingan Akurasi pada klasifikasi data Iris dengan menggunakan Decision Tree antara data numerik dan kategorical</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kesimpulan-perbandingan-akurasi-decision-tree-pada-data-iris">📊 Kesimpulan Perbandingan Akurasi Decision Tree pada Data Iris</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">✅ Kesimpulan:</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>