{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNYS8y/INSI2a7q4IW39lGV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Decision Tree\n","\n","Pohon Keputusan(Decision Tree adalah salah satu metode dalam penambangan data yang digunakan untuk klasifikasi dan regresi. Model ini sangat populer karena strukturnya yang intuitif, menyerupai diagram alir atau pohon, sehingga mudah dipahami bahkan oleh orang non-teknis.\n","\n","Secara sederhana, Decision Tree memetakan keputusan-keputusan yang mungkin diambil beserta konsekuensinya. Setiap simpul (node) dalam pohon merepresentasikan sebuah \"tes\" terhadap suatu atribut (fitur), setiap cabang (branch) merepresentasikan hasil dari tes tersebut, dan setiap daun (leaf node) merepresentasikan label kelas (keputusan akhir).\n","\n","Bagaimana Cara Kerjanya?\n","\n","Algoritma Decision Tree bekerja dengan cara mempartisi atau membagi data secara berulang-ulang berdasarkan fitur/atribut tertentu. Tujuannya adalah untuk membuat sub-grup data yang se-\"murni\" mungkin. \"Murni\" di sini berarti semua anggota dalam sub-grup tersebut memiliki kelas atau hasil yang sama.\n","\n","Proses utamanya adalah:\n","\n","- Pilih Atribut Terbaik: Algoritma dimulai dari Simpul Akar (Root Node) yang berisi seluruh dataset. Kemudian, ia mencari atribut mana yang paling baik dalam memisahkan data menjadi kelompok-kelompok yang paling murni. Untuk mengukur \"kemurnian\" ini, metrik seperti Gini Impurity atau Information Gain (Entropy) digunakan.\n","- Lakukan Pemisahan (Splitting): Dataset dibagi menjadi beberapa subset berdasarkan nilai dari atribut yang terpilih. Setiap subset ini akan menjadi cabang baru.\n","- Ulangi Proses: Proses nomor 1 dan 2 diulangi untuk setiap subset (yang kini menjadi simpul baru) sampai salah satu kondisi berikut terpenuhi:\n","1. Semua data dalam simpul sudah memiliki kelas yang sama (sudah murni).\n","2. Tidak ada lagi atribut yang bisa digunakan untuk memisahkan data.\n","3. Pohon sudah mencapai kedalaman maksimum yang ditentukan sebelumnya untuk mencegahnya menjadi terlalu rumit.\n","4. Simpul yang tidak bisa dibagi lagi disebut Simpul Daun (Leaf Node) dan berisi keputusan atau prediksi akhir.\n","\n","Istilah Penting dalam Decision Tree\n","- Simpul Akar (Root Node): Simpul paling atas yang mewakili seluruh populasi atau dataset.\n","- Simpul Keputusan (Decision Node): Simpul yang memiliki dua atau lebih cabang, merepresentasikan sebuah tes pada suatu atribut.\n","- Simpul Daun (Leaf Node): Simpul akhir yang tidak bisa dipecah lagi dan menunjukkan hasil/keputusan akhir (label kelas).\n","- Pemisahan (Splitting): Proses membagi sebuah simpul keputusan menjadi sub-simpul.\n","- Pemangkasan (Pruning): Proses menghilangkan cabang dari pohon keputusan untuk mengurangi kompleksitas dan mencegah overfitting. Overfitting adalah kondisi di mana model terlalu \"menghafal\" data latih sehingga performanya buruk saat dihadapkan pada data baru.\n","\n"],"metadata":{"id":"kVgtM4b9Ni6E"}},{"cell_type":"markdown","source":["data\n","\n","| No | Jam Belajar per Minggu | Nilai Ujian Sebelumnya | Status Kelulusan      |\n","|:---|:----------------------:|:----------------------:|:----------------------|\n","| 1  | 2                      | 55                     | Tidak Lulus           |\n","| 2  | 3                      | 50                     | Tidak Lulus           |\n","| 3  | 3                      | 45                     | Tidak Lulus           |\n","| 4  | 1                      | 40                     | Tidak Lulus           |\n","| 5  | 4                      | 59                     | Lulus                 |\n","| 6  | 8                      | 75                     | Lulus                 |\n","| 7  | 6                      | 65                     | Lulus                 |\n","| 8  | 10                     | 70                     | Lulus                 |\n","| 9  | 5                      | 80                     | Lulus                 |\n","| 10 | 9                      | 68                     | Lulus                 |\n","| 11 | 12                     | 85                     | Lulus dengan Pujian   |\n","| 12 | 15                     | 90                     | Lulus dengan Pujian   |\n","| 13 | 11                     | 92                     | Lulus dengan Pujian   |\n","| 14 | 14                     | 88                     | Lulus dengan Pujian   |\n","| 15 | 13                     | 82                     | Lulus dengan Pujian   |"],"metadata":{"id":"d6_QQjLMyhgT"}},{"cell_type":"markdown","source":["## Langkah-langkah Decision Tree\n","\n","1. **Menghitung Entropy Awal**\n","\n","Secara sederhana, Entropy adalah ukuran ketidakpastian, keacakan, atau ketidakteraturan dalam sekumpulan data. Semakin tinggi nilai Entropy, semakin acak dan tidak teratur data tersebut. Sebaliknya, semakin rendah nilai Entropy, semakin teratur dan mudah ditebak data tersebut.\n","\n","### Rumus Entropy\n","Rumus untuk menghitung Entropy adalah:\n","\n","$$Entropy(S) = \\sum_{i=1}^{c} -p_i \\log_2(p_i)$$\n","\n","Di mana:\n","* **$S$** = Keseluruhan dataset.\n","* **$c$** = Jumlah kelas unik (dalam kasus ini ada 3: Tidak Lulus, Lulus, Lulus dengan Pujian).\n","* **$p_i$** = Proporsi dari kelas ke-i dalam dataset(jumlah data kelas ke-i dibagi keseluruhan data).\n","\n","2. **Urutkan datanya(jika berupa numerik)**\n","  Pastikan untuk setiap fitur sudah berupa data yang terurut\n","3. **üìå Penjelasan Threshold dalam Decision Tree**\n","\n","**Threshold** dalam Decision Tree adalah batas nilai yang digunakan untuk memisahkan data pada atribut numerik (kontinu), seperti *Jam Belajar* atau *Nilai Ujian*. Tujuannya adalah membagi data ke dalam dua kelompok yang sehomogen mungkin terhadap label atau kelas (misalnya: Tidak Lulus, Lulus, Lulus dengan Pujian).\n","\n","\n","Jika kita bisa membagi data dengan threshold sehingga:\n","\n","* satu sisi didominasi oleh satu kelas tertentu (contoh: semua \"Tidak Lulus\"),\n","* dan sisi lainnya oleh kelas lain (contoh: semua \"Lulus\"),\n","\n","...maka model lebih akurat dan pohonnya lebih sederhana.\n","\n","---\n","\n","### üßÆ Rumus Threshold\n","\n","Threshold dicari di **titik tengah antara dua nilai berurutan yang memiliki kelas berbeda**. Ini disebut sebagai **midpoint threshold**.\n","\n","#### ‚úÖ Rumus Threshold:\n","\n","Jika kamu memiliki dua nilai `v‚ÇÅ` dan `v‚ÇÇ` (urutan setelah sorting), maka:\n","\n","$$\n","\\text{Threshold} = \\frac{v_1 + v_2}{2}\n","$$\n","\n","Dengan syarat bahwa nilai `v‚ÇÅ` dan `v‚ÇÇ` berasal dari **kelas yang berbeda**.\n","\n","---\n","\n","### üßë‚Äçüè´ Contoh:\n","\n","Misalkan data `Nilai Ujian` yang sudah diurutkan:\n","\n","```\n","40(A), 45(A), 50(A), 55(A), 59(B), 65(B), ...\n","```\n","\n","Lihat perubahan kelas dari A ‚Üí B di antara **55 dan 59**. Maka:\n","\n","$$\n","\\text{Threshold} = \\frac{55 + 59}{2} = 57\n","$$\n","\n","Threshold ini diuji untuk split: `nilai < 57` vs `nilai ‚â• 57`.\n","\n","4. **hitung entropy dari masing masing thresholdnya**\n","\n","Berikut adalah **penjelasan lengkap** dan **rumus** dari proses **menghitung entropy split** dalam decision tree (C4.5 atau ID3), berdasarkan contoh yang kamu berikan:\n","\n","---\n","\n","#### ‚úÖ APA ITU ENTROPY SPLIT?\n","\n","**Entropy Split** digunakan untuk mengukur **ketidakpastian rata-rata** dari pembagian data ke dalam dua subset berdasarkan **threshold tertentu** dari sebuah atribut.\n","\n","Tujuan dari menghitung entropy split adalah untuk mengetahui **seberapa ‚Äúbersih‚Äù pembagian data** berdasarkan threshold tertentu. Semakin kecil nilai entropy split, semakin baik pembagiannya.\n","\n","---\n","\n","#### ‚úÖ LANGKAH-LANGKAH + RUMUS\n","\n","##### **1. Tentukan Threshold dan Bagi Data**\n","\n","Misalnya, kita pilih atribut **Nilai Ujian**, lalu threshold-nya adalah **57**.\n","\n","Maka:\n","\n","* **S ‚â§ 57** ‚Üí Data dengan nilai ujian ‚â§ 57\n","* **S > 57** ‚Üí Data dengan nilai ujian > 57\n","\n","Contoh:\n","\n","* S ‚â§ 57 ‚Üí 4 data (semua dari kelas A)\n","* S > 57 ‚Üí 11 data (6 dari kelas B, 5 dari kelas C)\n","\n","---\n","\n","##### **2. Hitung Entropy Masing-Masing Subset**\n","\n","###### üîπ a. Entropy untuk S ‚â§ 57\n","\n","Karena semua data di subset ini berasal dari satu kelas (**kelas A**), maka entropinya **0** (tidak ada ketidakpastian):\n","\n","$$\n","\\text{Entropy}(S_1) = 0\n","$$\n","\n","---\n","\n","#### üîπ b. Entropy untuk S > 57\n","\n","Ada dua kelas: B dan C.\n","\n","* Jumlah total data: 11\n","* Jumlah data kelas B: 6 ‚Üí $p_B = \\frac{6}{11}$\n","* Jumlah data kelas C: 5 ‚Üí $p_C = \\frac{5}{11}$\n","\n","$$\n","\\text{Entropy}(S_2) = -p_B \\cdot \\log_2(p_B) - p_C \\cdot \\log_2(p_C)\n","$$\n","\n","$$\n","= -\\left( \\frac{6}{11} \\log_2 \\frac{6}{11} + \\frac{5}{11} \\log_2 \\frac{5}{11} \\right)\n","$$\n","\n","Hitung:\n","\n","$$\n","\\frac{6}{11} ‚âà 0.545,\\quad \\log_2(0.545) ‚âà -0.874\n","$$\n","\n","$$\n","\\frac{5}{11} ‚âà 0.455,\\quad \\log_2(0.455) ‚âà -1.137\n","$$\n","\n","$$\n","= -\\left( 0.545 \\cdot (-0.874) + 0.455 \\cdot (-1.137) \\right)\n","= 0.476 + 0.518 = 0.994\n","$$\n","\n","---\n","\n","4. **Hitung Entropy Split Total**\n","\n","$$\n","\\text{Entropy}_{split} = \\frac{|S_1|}{|S|} \\cdot \\text{Entropy}(S_1) + \\frac{|S_2|}{|S|} \\cdot \\text{Entropy}(S_2)\n","$$\n","\n","$$\n","= \\frac{4}{15} \\cdot 0 + \\frac{11}{15} \\cdot 0.994 = 0.7293\n","$$\n","\n","5. ** hitung information gainnya**\n","Baik! Sekarang kita akan membahas langkah ke-5 dalam membangun decision tree, yaitu **menghitung Information Gain (IG)**. Di bawah ini penjelasan lengkapnya:\n","\n","---\n","\n","##### ‚úÖ **Apa itu Information Gain?**\n","\n","**Information Gain (IG)** mengukur **seberapa besar pengurangan entropi** yang diperoleh dengan membagi data berdasarkan fitur tertentu.\n","\n","Artinya: Semakin tinggi nilai IG, semakin baik fitur tersebut dalam **memisahkan kelas** (misalnya, ‚ÄúTidak Lulus‚Äù, ‚ÄúLulus‚Äù, ‚ÄúLulus dengan Pujian‚Äù).\n","\n","---\n","\n","##### ‚úÖ **Rumus Information Gain**\n","\n","$$\n","\\text{Information Gain (IG)} = \\text{Entropy Awal (S)} - \\text{Entropy Split (S|A)}\n","$$\n","\n","Keterangan:\n","\n","* **Entropy(S)**: Entropi seluruh dataset sebelum dibagi.\n","* **Entropy(S|A)**: Entropi rata-rata setelah data dibagi berdasarkan atribut A.\n","\n","---\n","\n","##### ‚úÖ **Langkah Perhitungan (Contoh Kasus)**\n","\n","Misalkan:\n","\n","* Entropy awal seluruh dataset (sebelum split):\n","\n","  $$\n","  \\text{Entropy}(S) = 1.566\n","  $$\n","\n","* Entropy split setelah membagi berdasarkan atribut **nilai ujian dengan threshold 57**:\n","\n","  $$\n","  \\text{Entropy}_{split} = 0.7293\n","  $$\n","\n","---\n","\n","##### ‚úÖ **Hitung Information Gain**\n","\n","$$\n","\\text{IG} = 1.566 - 0.7293 = 0.8367\n","$$\n","\n","---\n","\n","##### ‚úÖ **Kesimpulan**\n","\n","* Nilai **Information Gain = 0.8367**\n","* Karena nilai ini **cukup tinggi**, maka atribut ‚ÄúNilai Ujian‚Äù dengan threshold 57 adalah pembagi yang baik.\n","* Jika kamu membandingkan beberapa fitur, **pilih fitur dengan IG tertinggi** untuk menjadi **node cabang utama** pada decision tree.\n","\n","5. **Pilih Atribut dengan Information Gain Tertinggi**\n","\n","Fitur dengan IG tertinggi dipilih sebagai atribut pembagi (node).\n","\n","Threshold-nya digunakan untuk memisahkan data ke dalam cabang-cabang (misalnya, <= threshold dan > threshold).\n","\n","üìå Contoh:\n","Jika \"Nilai Ujian\" dengan threshold 57 memiliki IG tertinggi, maka:\n","\n","Node decision tree ‚Üí \"Nilai Ujian <= 57?\"\n","\n","Cabang kiri (yes): Data dengan nilai ‚â§ 57\n","\n","Cabang kanan (no): Data dengan nilai > 57\n","\n","6. **buat cabang dan rekursif ulangi dari langkah 3\n","\n","Untuk masing-masing cabang (hasil split):\n","\n","a. Cek apakah entropi = 0 (pure):\n","Jika ya ‚Üí node menjadi leaf (kelas mayoritas di data cabang).\n","\n","Misalnya, jika semua datanya ‚ÄúTidak Lulus‚Äù, maka labelnya langsung = ‚ÄúTidak Lulus‚Äù.\n","\n","b. Jika entropi ‚â† 0 (belum pure):\n","Ulangi proses dari langkah 2 hingga 6 di subset data cabang itu:\n","\n","Hitung entropy\n","\n","Hitung IG semua fitur yang belum dipakai\n","\n","Pilih IG tertinggi\n","\n","Split lagi data\n","\n","Lanjutkan hingga semua data terklasifikasi dengan baik atau fitur habis\n","\n","7. **Selesai Jika Semua Data Terklasifikasi\n","\n","Decision tree selesai jika:\n","\n","* Semua node menjadi leaf (kelas sudah pure), atau\n","\n","* Tidak ada lagi fitur yang bisa digunakan\n","\n","\n","\n"],"metadata":{"id":"sRvSnyY4xzvG"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier, export_graphviz\n","from sklearn.preprocessing import LabelEncoder\n","import graphviz\n","\n","# 1. Menyiapkan Data\n","# Membuat DataFrame dari data terakhir yang Anda berikan.\n","data = {\n","    'Jam Belajar per Minggu': [2, 3, 3, 1, 4, 8, 6, 10, 5, 9, 12, 15, 11, 14, 13],\n","    'Nilai Ujian Sebelumnya': [55, 50, 45, 40, 59, 75, 65, 70, 80, 68, 85, 90, 92, 88, 82],\n","    'Status Kelulusan': [\n","        'Tidak Lulus', 'Tidak Lulus', 'Tidak Lulus', 'Tidak Lulus', 'Lulus',\n","        'Lulus', 'Lulus', 'Lulus', 'Lulus', 'Lulus',\n","        'Lulus dengan Pujian', 'Lulus dengan Pujian', 'Lulus dengan Pujian',\n","        'Lulus dengan Pujian', 'Lulus dengan Pujian'\n","    ]\n","}\n","df = pd.DataFrame(data)\n","\n","# 2. Preprocessing Data\n","# Memisahkan fitur (X) dan target (y)\n","X = df[['Jam Belajar per Minggu', 'Nilai Ujian Sebelumnya']]\n","y = df['Status Kelulusan']\n","\n","# Mengubah label target dari teks menjadi angka (0, 1, 2)\n","# karena model memerlukan input numerik.\n","le = LabelEncoder()\n","y_encoded = le.fit_transform(y)\n","# Mengetahui kelas asli dari label yang di-encode:\n","# 0: Lulus, 1: Lulus dengan Pujian, 2: Tidak Lulus\n","# (urutan berdasarkan abjad)\n","class_names_original = le.classes_\n","\n","# 3. Membuat dan Melatih Model Decision Tree\n","# Membuat model. max_depth=3 berarti pohon tidak akan lebih dari 3 tingkat,\n","# untuk mencegah terlalu rumit dan mudah dibaca.\n","# criterion='entropy' agar sesuai dengan perhitungan manual kita.\n","model = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)\n","\n","# Melatih model dengan seluruh data\n","model.fit(X, y_encoded)\n","\n","print(\"Model Decision Tree berhasil dilatih.\")\n","\n","# 4. Visualisasi Pohon Keputusan\n","# Mengekspor model ke dalam format DOT\n","dot_data = export_graphviz(\n","    model,\n","    out_file=None,\n","    feature_names=X.columns,\n","    class_names=class_names_original,\n","    filled=True,\n","    rounded=True,\n","    special_characters=True\n",")\n","\n","# Menampilkan grafik di notebook\n","graph = graphviz.Source(dot_data)\n","print(\"\\nVisualisasi Pohon Keputusan:\")\n","# Untuk menyimpan gambar sebagai file PNG\n","# graph.render(\"decision_tree_kelulusan\")\n","graph"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":494},"id":"Gmr71ZbUPnz9","executionInfo":{"status":"ok","timestamp":1749651177596,"user_tz":-420,"elapsed":6814,"user":{"displayName":"23-175 AHMAD DHIYAUDDIN","userId":"05009657224641630558"}},"outputId":"5643d8d4-6158-4233-d34f-a46425da0cc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Decision Tree berhasil dilatih.\n","\n","Visualisasi Pohon Keputusan:\n"]},{"output_type":"execute_result","data":{"image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"446pt\" height=\"314pt\"\n viewBox=\"0.00 0.00 445.50 314.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-310 441.5,-310 441.5,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#fcf2eb\" stroke=\"black\" d=\"M329,-306C329,-306 151,-306 151,-306 145,-306 139,-300 139,-294 139,-294 139,-235 139,-235 139,-229 145,-223 151,-223 151,-223 329,-223 329,-223 335,-223 341,-229 341,-235 341,-235 341,-294 341,-294 341,-300 335,-306 329,-306\"/>\n<text text-anchor=\"start\" x=\"147\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Jam Belajar per Minggu ‚â§ 10.5</text>\n<text text-anchor=\"start\" x=\"192.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 1.566</text>\n<text text-anchor=\"start\" x=\"199\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 15</text>\n<text text-anchor=\"start\" x=\"193\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [6, 5, 4]</text>\n<text text-anchor=\"start\" x=\"200.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Lulus</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#f6d5bd\" stroke=\"black\" d=\"M222,-187C222,-187 48,-187 48,-187 42,-187 36,-181 36,-175 36,-175 36,-116 36,-116 36,-110 42,-104 48,-104 48,-104 222,-104 222,-104 228,-104 234,-110 234,-116 234,-116 234,-175 234,-175 234,-181 228,-187 222,-187\"/>\n<text text-anchor=\"start\" x=\"44\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Nilai Ujian Sebelumnya ‚â§ 57.0</text>\n<text text-anchor=\"start\" x=\"87.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.971</text>\n<text text-anchor=\"start\" x=\"94\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 10</text>\n<text text-anchor=\"start\" x=\"88\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [6, 0, 4]</text>\n<text text-anchor=\"start\" x=\"95.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Lulus</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M203.57,-222.91C195.35,-213.74 186.54,-203.93 178.07,-194.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"180.65,-192.13 171.36,-187.02 175.44,-196.8 180.65,-192.13\"/>\n<text text-anchor=\"middle\" x=\"170.02\" y=\"-208.28\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M425.5,-179.5C425.5,-179.5 264.5,-179.5 264.5,-179.5 258.5,-179.5 252.5,-173.5 252.5,-167.5 252.5,-167.5 252.5,-123.5 252.5,-123.5 252.5,-117.5 258.5,-111.5 264.5,-111.5 264.5,-111.5 425.5,-111.5 425.5,-111.5 431.5,-111.5 437.5,-117.5 437.5,-123.5 437.5,-123.5 437.5,-167.5 437.5,-167.5 437.5,-173.5 431.5,-179.5 425.5,-179.5\"/>\n<text text-anchor=\"start\" x=\"305\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"307.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\n<text text-anchor=\"start\" x=\"298\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 5, 0]</text>\n<text text-anchor=\"start\" x=\"260.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Lulus dengan Pujian</text>\n</g>\n<!-- 0&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>0&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M276.43,-222.91C286.83,-211.32 298.16,-198.7 308.55,-187.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"311.16,-189.45 315.23,-179.67 305.95,-184.77 311.16,-189.45\"/>\n<text text-anchor=\"middle\" x=\"316.58\" y=\"-200.93\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M120,-68C120,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,0 12,0 12,0 120,0 120,0 126,0 132,-6 132,-12 132,-12 132,-56 132,-56 132,-62 126,-68 120,-68\"/>\n<text text-anchor=\"start\" x=\"26\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"28.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\n<text text-anchor=\"start\" x=\"19\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 4]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Tidak Lulus</text>\n</g>\n<!-- 1&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>1&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M109.31,-103.73C103.79,-94.97 97.95,-85.7 92.41,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"95.28,-74.89 86.98,-68.3 89.35,-78.63 95.28,-74.89\"/>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M248,-68C248,-68 162,-68 162,-68 156,-68 150,-62 150,-56 150,-56 150,-12 150,-12 150,-6 156,0 162,0 162,0 248,0 248,0 254,0 260,-6 260,-12 260,-12 260,-56 260,-56 260,-62 254,-68 248,-68\"/>\n<text text-anchor=\"start\" x=\"165\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"167.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [6, 0, 0]</text>\n<text text-anchor=\"start\" x=\"165.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Lulus</text>\n</g>\n<!-- 1&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>1&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M161.07,-103.73C166.66,-94.97 172.59,-85.7 178.21,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"181.28,-78.61 183.71,-68.3 175.38,-74.84 181.28,-78.61\"/>\n</g>\n</g>\n</svg>\n","text/plain":["<graphviz.sources.Source at 0x7c488ff3a4d0>"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":[],"metadata":{"id":"z2cLuocvMkI0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CuETE-vqMast"},"outputs":[],"source":[]}]}